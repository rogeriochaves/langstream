{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec042b3",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "---\n",
    "\n",
    "# Simple Bot with Weather Tool and Error Handling\n",
    "\n",
    "The example below is similar to the [Simple Bot with Weather Tool](./weather-bot) example, but here we add `on_error` for error handling in case something went wrong when calling the weather function. What we do is simply inject the error back into the LLM, so it can figure out itself what is missing, which in example below is the `location` field, and ask the user for more input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d41fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, AsyncGenerator, List, Literal, Tuple, TypedDict\n",
    "from litechain import debug, as_async_generator\n",
    "\n",
    "from litechain.contrib.llms.open_ai import (\n",
    "    OpenAIChatChain,\n",
    "    OpenAIChatDelta,\n",
    "    OpenAIChatMessage,\n",
    ")\n",
    "from litechain.core.chain import Chain, ChainOutput\n",
    "\n",
    "\n",
    "class Memory(TypedDict):\n",
    "    history: List[OpenAIChatMessage]\n",
    "\n",
    "\n",
    "memory = Memory(history=[])\n",
    "\n",
    "\n",
    "def save_message_to_memory(message: OpenAIChatMessage) -> OpenAIChatMessage:\n",
    "    memory[\"history\"].append(message)\n",
    "    return message\n",
    "\n",
    "\n",
    "def update_delta_on_memory(delta: OpenAIChatDelta) -> OpenAIChatDelta:\n",
    "    if not isinstance(delta, OpenAIChatDelta):\n",
    "        return delta\n",
    "\n",
    "    if memory[\"history\"][-1].role != delta.role and delta.role is not None:\n",
    "        memory[\"history\"].append(\n",
    "            OpenAIChatMessage(role=delta.role, content=delta.content, name=delta.name)\n",
    "        )\n",
    "    else:\n",
    "        memory[\"history\"][-1].content += delta.content\n",
    "    return delta\n",
    "\n",
    "\n",
    "def get_current_weather(\n",
    "    location: str, format: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\"\n",
    ") -> OpenAIChatDelta:\n",
    "    result = {\n",
    "        \"location\": location,\n",
    "        \"forecast\": \"sunny\",\n",
    "        \"temperature\": \"25 C\" if format == \"celsius\" else \"77 F\",\n",
    "    }\n",
    "\n",
    "    return OpenAIChatDelta(\n",
    "        role=\"function\", name=\"get_current_weather\", content=json.dumps(result)\n",
    "    )\n",
    "\n",
    "\n",
    "def error_handler(\n",
    "    err: Exception,\n",
    ") -> AsyncGenerator[ChainOutput[OpenAIChatDelta], Any]:\n",
    "    # Try to recover from the error if it happened on the function calling\n",
    "    if \"get_current_weather\" in str(err):\n",
    "        x = function_error_chain((\"get_current_weather\", err))\n",
    "        return x\n",
    "    else:\n",
    "        # Otherwise just re-raise it\n",
    "        raise err\n",
    "\n",
    "\n",
    "# Chain Definitions\n",
    "\n",
    "weather_chain = debug(\n",
    "    OpenAIChatChain[str, OpenAIChatDelta](\n",
    "        \"WeatherChain\",\n",
    "        lambda user_input: [\n",
    "            OpenAIChatMessage(\n",
    "                role=\"system\",\n",
    "                content=\"You are a chatbot that has access to real-time weather information\",\n",
    "            ),\n",
    "            *memory[\"history\"],\n",
    "            save_message_to_memory(\n",
    "                OpenAIChatMessage(role=\"user\", content=user_input),\n",
    "            ),\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Gets the current weather in a given location, use this function for any questions related to the weather\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"description\": \"The city to get the weather, e.g. San Francisco. Guess the location from user messages\",\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"format\": {\n",
    "                            \"description\": \"A string with the full content of what the given role said\",\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": (\"celsius\", \"fahrenheit\"),\n",
    "                        },\n",
    "                    },\n",
    "                    # highlight-next-line\n",
    "                    # We comment this out so the model can send empty location by mistake\n",
    "                    # highlight-next-line\n",
    "                    # \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    .map(\n",
    "        # We store the function call that the LLM made with it's arguments in memory, so it can inspect itself back later on\n",
    "        lambda delta: save_message_to_memory(\n",
    "            OpenAIChatMessage(\n",
    "                role=\"function\",\n",
    "                name=\"get_current_weather\",\n",
    "                content=delta.content,\n",
    "            )\n",
    "        )\n",
    "        # Then we call the function\n",
    "        and get_current_weather(**json.loads(delta.content))\n",
    "        # If it was called of course\n",
    "        if delta.role == \"function\" and delta.name == \"get_current_weather\"\n",
    "        else delta\n",
    "    )\n",
    "    # highlight-next-line\n",
    "    .on_error(error_handler)\n",
    "    .map(update_delta_on_memory)\n",
    ")\n",
    "\n",
    "function_reply_chain = debug(\n",
    "    OpenAIChatChain[None, OpenAIChatDelta](\n",
    "        \"FunctionReplyChain\",\n",
    "        lambda _: memory[\"history\"],\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        temperature=0,\n",
    "    ).map(update_delta_on_memory)\n",
    ")\n",
    "\n",
    "# If an error happens, this chain is triggered, it simply takes the current history, plus a user message with the error message\n",
    "# this is enough for the model to figure out what was the issue and ask user for additional input\n",
    "function_error_chain = OpenAIChatChain[Tuple[str, Exception], OpenAIChatDelta](\n",
    "    \"FunctionErrorChain\",\n",
    "    lambda name_and_err: [\n",
    "        *memory[\"history\"],\n",
    "        save_message_to_memory(\n",
    "            OpenAIChatMessage(\n",
    "                role=\"user\",\n",
    "                content=str(name_and_err[1]),\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "weather_bot: Chain[str, OpenAIChatDelta] = weather_chain.and_then(\n",
    "    # Reply based on function result if last output was a function output\n",
    "    lambda outputs: function_reply_chain(None)\n",
    "    if list(outputs)[-1].role == \"function\"\n",
    "    # Otherwise just re-yield the outputs\n",
    "    else as_async_generator(*outputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731c90c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "from litechain.utils.chain import collect_final_output\n",
    "\n",
    "_ = await collect_final_output(weather_bot(\"hi there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4aa8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mFunction get_current_weather:\u001b[39m {}\n",
      "\n",
      "\u001b[32m> WeatherChain@map\u001b[39m\n",
      "\n",
      "\u001b[31mException:\u001b[39m get_current_weather() missing 1 required positional argument: 'location'\n",
      "\n",
      "\u001b[32m> FunctionErrorChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m I apologize for the inconvenience. In order to provide you with the current weather, could you please provide me with your location?"
     ]
    }
   ],
   "source": [
    "from litechain.utils.chain import collect_final_output\n",
    "\n",
    "_ = await collect_final_output(weather_bot(\"it is hot today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a44fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mFunction get_current_weather:\u001b[39m {\n",
      "  \"location\": \"Amsterdam\"\n",
      "}\n",
      "\n",
      "\u001b[32m> WeatherChain@map\u001b[39m\n",
      "\n",
      "\u001b[33mFunction get_current_weather:\u001b[39m {\"location\": \"Amsterdam\", \"forecast\": \"sunny\", \"temperature\": \"25 C\"}\n",
      "\n",
      "\u001b[32m> FunctionReplyChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m It seems that the current weather in Amsterdam is sunny with a temperature of 25°C. Stay hydrated and enjoy the day!"
     ]
    }
   ],
   "source": [
    "_ = await collect_final_output(weather_bot(\"I am in Amsterdam\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f25374",
   "metadata": {},
   "source": [
    "As you can see, the bot first tried to call `get_current_weather` with empty arguments, which threw an error, we inject this error back into the `FunctionErrorChain`, making the bot realize the mistake and ask the user to provide the location. Once provided, the function call is triggered again, this time with the right location and response.\n",
    "\n",
    "Now take a look on what happened inside the memory, we save both the original function call and the error message there:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a13c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OpenAIChatMessage(role='user', content='hi there', name=None),\n",
       " OpenAIChatMessage(role='assistant', content='Hello! How can I assist you today?', name=None),\n",
       " OpenAIChatMessage(role='user', content='it is hot today?', name=None),\n",
       " OpenAIChatMessage(role='function', content='{}', name='get_current_weather'),\n",
       " OpenAIChatMessage(role='user', content=\"get_current_weather() missing 1 required positional argument: 'location'\", name=None),\n",
       " OpenAIChatMessage(role='assistant', content='I apologize for the inconvenience. In order to provide you with the current weather, could you please provide me with your location?', name=None),\n",
       " OpenAIChatMessage(role='user', content='I am in Amsterdam', name=None),\n",
       " OpenAIChatMessage(role='function', content='{\\n  \"location\": \"Amsterdam\"\\n}{\"location\": \"Amsterdam\", \"forecast\": \"sunny\", \"temperature\": \"25 C\"}', name='get_current_weather'),\n",
       " OpenAIChatMessage(role='assistant', content='It seems that the current weather in Amsterdam is sunny with a temperature of 25°C. Stay hydrated and enjoy the day!', name=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory[\"history\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "862136fe",
   "metadata": {},
   "source": [
    "That's it, if you have any questions about this example, [join our discord community](https://discord.gg/48ZM5KkKgw) and we can help you out.\n",
    "\n",
    "Also, if you are interested in running a bot like this inside a nice UI, check out our [docs on Chainlit](../ui/chainlit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82d862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
