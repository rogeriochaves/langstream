{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec042b3",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---\n",
    "\n",
    "# Simple Bot with Weather Tool\n",
    "\n",
    "Below is a code example of a bot you can talk too which has the ability of checking the weather, it has memory, it is using OpenAI functions, and it streams its outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d41fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Literal, TypedDict\n",
    "from litechain import Chain, debug, as_async_generator\n",
    "\n",
    "from litechain.contrib.llms.open_ai import (\n",
    "    OpenAIChatChain,\n",
    "    OpenAIChatDelta,\n",
    "    OpenAIChatMessage,\n",
    ")\n",
    "\n",
    "\n",
    "class Memory(TypedDict):\n",
    "    history: List[OpenAIChatMessage]\n",
    "\n",
    "\n",
    "memory = Memory(history=[])\n",
    "\n",
    "\n",
    "def save_message_to_memory(message: OpenAIChatMessage) -> OpenAIChatMessage:\n",
    "    memory[\"history\"].append(message)\n",
    "    return message\n",
    "\n",
    "\n",
    "def update_delta_on_memory(delta: OpenAIChatDelta) -> OpenAIChatDelta:\n",
    "    if memory[\"history\"][-1].role != delta.role and delta.role is not None:\n",
    "        memory[\"history\"].append(\n",
    "            OpenAIChatMessage(role=delta.role, content=delta.content, name=delta.name)\n",
    "        )\n",
    "    else:\n",
    "        memory[\"history\"][-1].content += delta.content\n",
    "    return delta\n",
    "\n",
    "\n",
    "def get_current_weather(\n",
    "    location: str, format: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\"\n",
    ") -> OpenAIChatDelta:\n",
    "    result = {\n",
    "        \"location\": location,\n",
    "        \"forecast\": \"sunny\",\n",
    "        \"temperature\": \"25 C\" if format == \"celsius\" else \"77 F\",\n",
    "    }\n",
    "\n",
    "    return OpenAIChatDelta(\n",
    "        role=\"function\", name=\"get_current_weather\", content=json.dumps(result)\n",
    "    )\n",
    "\n",
    "\n",
    "# Chain Definitions\n",
    "\n",
    "weather_chain = (\n",
    "    debug(\n",
    "        OpenAIChatChain[str, OpenAIChatDelta](\n",
    "            \"WeatherChain\",\n",
    "            lambda user_input: [\n",
    "                *memory[\"history\"],\n",
    "                save_message_to_memory(\n",
    "                    OpenAIChatMessage(role=\"user\", content=user_input),\n",
    "                ),\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            functions=[\n",
    "                {\n",
    "                    \"name\": \"get_current_weather\",\n",
    "                    \"description\": \"Gets the current weather in a given location, use this function for any questions related to the weather\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"description\": \"The city to get the weather, e.g. San Francisco. Guess the location from user messages\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"description\": \"A string with the full content of what the given role said\",\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": (\"celsius\", \"fahrenheit\"),\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\"],\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "    )\n",
    "    .map(\n",
    "        # Call the function if the model produced a function call by parsing the json arguments\n",
    "        lambda delta: get_current_weather(**json.loads(delta.content))\n",
    "        if delta.role == \"function\" and delta.name == \"get_current_weather\"\n",
    "        else delta\n",
    "    )\n",
    "    .map(update_delta_on_memory)\n",
    ")\n",
    "\n",
    "function_reply_chain = debug(\n",
    "    OpenAIChatChain[None, OpenAIChatDelta](\n",
    "        \"FunctionReplyChain\",\n",
    "        lambda _: memory[\"history\"],\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        temperature=0,\n",
    "    )\n",
    ").map(update_delta_on_memory)\n",
    "\n",
    "weather_bot: Chain[str, OpenAIChatDelta] = weather_chain.and_then(\n",
    "    # Reply based on function result if last output was a function output\n",
    "    lambda outputs: function_reply_chain(None)\n",
    "    if list(outputs)[-1].role == \"function\"\n",
    "    # Otherwise just re-yield the outputs\n",
    "    else as_async_generator(*outputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731c90c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "from litechain.utils.chain import collect_final_output\n",
    "\n",
    "_ = await collect_final_output(weather_bot(\"hi there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a44fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mFunction get_current_weather:\u001b[39m {\n",
      "  \"location\": \"Amsterdam\"\n",
      "}\n",
      "\n",
      "\u001b[32m> FunctionReplyChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m Yes, it is hot today in Amsterdam. The current temperature is 25°C and it is sunny."
     ]
    }
   ],
   "source": [
    "_ = await collect_final_output(weather_bot(\"is it hot today in Amsterdam?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f25374",
   "metadata": {},
   "source": [
    "The bot is working well, it replies chit-chat messages as well as calling the weather function when needed, and replying to the user in natural language.\n",
    "\n",
    "Let's inspect what's inside the bot memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a13c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OpenAIChatMessage(role='user', content='hi there', name=None),\n",
       " OpenAIChatMessage(role='assistant', content='Hello! How can I assist you today?', name=None),\n",
       " OpenAIChatMessage(role='user', content='is it hot today in Amsterdam?', name=None),\n",
       " OpenAIChatMessage(role='function', content='{\"location\": \"Amsterdam\", \"forecast\": \"sunny\", \"temperature\": \"25 C\"}', name='get_current_weather'),\n",
       " OpenAIChatMessage(role='assistant', content='Yes, it is hot today in Amsterdam. The current temperature is 25°C and it is sunny.', name=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory['history']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "862136fe",
   "metadata": {},
   "source": [
    "It saved both the conversation and the results of the function call, this way, continued conversations will be able to use the previous context, include the previous function result.\n",
    "\n",
    "That's it, if you have any questions about this example, [join our discord community](https://discord.gg/48ZM5KkKgw) and we can help you out.\n",
    "\n",
    "Also, if you are interested in running a bot like this inside a nice UI, check out our [docs on Chainlit](../ui/chainlit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82d862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
