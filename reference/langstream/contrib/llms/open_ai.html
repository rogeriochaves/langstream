<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>langstream.contrib.llms.open_ai API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü™Ω</text></svg>" data-rh="true">
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/langstream/"
><b class="navbar__title text--truncate">ü™Ωüîó LangStream</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/langstream/docs/intro"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/langstream/reference/langstream/index.html"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/rogeriochaves/langstream"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>langstream.contrib.llms.open_ai</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import asyncio
from dataclasses import dataclass
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    TypeVar,
    Union,
    cast,
)

import importlib
from colorama import Fore
from retry import retry

from langstream.core.stream import Stream, StreamOutput

T = TypeVar(&#34;T&#34;)
U = TypeVar(&#34;U&#34;)
V = TypeVar(&#34;V&#34;)


class OpenAICompletionStream(Stream[T, U]):
    &#34;&#34;&#34;
    `OpenAICompletionStream` uses the most simple LLMs from OpenAI based on GPT-3 for text completion, if you are looking for ChatCompletion, take a look at `OpenAIChatStream`.

    The `OpenAICompletionStream` takes a lambda function that should return a string with the prompt for completion.

    To use this stream you will need an `OPENAI_API_KEY` environment variable to be available, and then you can generate completions out of it.

    You can read more about the completion API on [OpenAI API reference](https://platform.openai.com/docs/api-reference/completions)

    Example
    -------

    &gt;&gt;&gt; from langstream import join_final_output
    &gt;&gt;&gt; from langstream.contrib import OpenAICompletionStream
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_stream = OpenAICompletionStream[str, str](
    ...         &#34;RecipeStream&#34;,
    ...         lambda recipe_name: f&#34;Here is my {recipe_name} recipe: &#34;,
    ...         model=&#34;ada&#34;,
    ...     )
    ...
    ...     return await join_final_output(recipe_stream(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#39;„ÄêInstant Noodles„Äë\\n\\nIngredients:\\n\\n1 cup of water&#39;

    &#34;&#34;&#34;

    def __init__(
        self: &#34;OpenAICompletionStream[T, str]&#34;,
        name: str,
        call: Callable[
            [T],
            str,
        ],
        model: str,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
        timeout: int = 5,
        retries: int = 3,
    ) -&gt; None:
        async def completion(prompt: str) -&gt; AsyncGenerator[U, None]:
            loop = asyncio.get_event_loop()

            @retry(tries=retries)
            def get_completions():
                openai = importlib.import_module(&#34;openai&#34;)
                client = openai.OpenAI()
                return client.completions.create(
                    model=model,
                    prompt=prompt,
                    temperature=temperature,
                    stream=True,
                    max_tokens=max_tokens,
                    timeout=timeout,
                )

            completions = await loop.run_in_executor(None, get_completions)

            for output in completions:
                output = cast(dict, output.model_dump())
                if &#34;choices&#34; in output:
                    if len(output[&#34;choices&#34;]) &gt; 0:
                        if &#34;text&#34; in output[&#34;choices&#34;][0]:
                            yield output[&#34;choices&#34;][0][&#34;text&#34;]

        super().__init__(name, lambda input: completion(call(input)))


@dataclass
class OpenAIChatMessage:
    &#34;&#34;&#34;
    OpenAIChatMessage is a data class that represents a chat message for building `OpenAIChatStream` prompt.

    Attributes
    ----------
    role : Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
        The role of who sent this message in the chat, can be one of `&#34;system&#34;`, `&#34;user&#34;`, `&#34;assistant&#34;` or &#34;function&#34;

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the full content of what the given role said

    &#34;&#34;&#34;

    role: Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
    content: str
    name: Optional[str] = None

    def to_dict(self):
        return {k: v for k, v in self.__dict__.items() if v is not None}


@dataclass
class OpenAIChatDelta:
    &#34;&#34;&#34;
    OpenAIChatDelta is a data class that represents the output of an `OpenAIChatStream`.

    Attributes
    ----------
    role : Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
        The role of the output message, the first message will have the role, while
        the subsequent partial content output ones will have the role as `None`.
        For now the only possible values it will have is either None or `&#34;assistant&#34;`

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the partial content being outputted by the LLM, this generally
        translate to each token the LLM is producing

    &#34;&#34;&#34;

    role: Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
    content: str
    name: Optional[str] = None

    def __stream_debug__(self):
        name = &#34;&#34;
        if self.name:
            name = f&#34; {self.name}&#34;
        if self.role is not None:
            print(f&#34;{Fore.YELLOW}{self.role.capitalize()}{name}:{Fore.RESET} &#34;, end=&#34;&#34;)
        print(
            self.content,
            end=&#34;&#34;,
            flush=True,
        )


class OpenAIChatStream(Stream[T, U]):
    &#34;&#34;&#34;
    `OpenAIChatStream` gives you access to the more powerful LLMs from OpenAI, like `gpt-3.5-turbo` and `gpt-4`, they are structured in a chat format with roles.

    The `OpenAIChatStream` takes a lambda function that should return a list of `OpenAIChatMessage` for the assistant to reply, it is stateless, so it doesn&#39;t keep
    memory of the past chat messages, you will have to handle the memory yourself, you can [follow this guide to get started on memory](https://rogeriochaves.github.io/langstream/docs/llms/memory).

    The `OpenAIChatStream` also produces `OpenAIChatDelta` as output, one per token, it contains the `role` that started the output, and then subsequent `content` updates.
    If you want the final content as a string, you will need to use the `.content` property from the delta and accumulate it for the final result.

    To use this stream you will need an `OPENAI_API_KEY` environment variable to be available, and then you can generate chat completions out of it.

    You can read more about the chat completion API on [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat)

    Example
    -------

    &gt;&gt;&gt; from langstream import Stream, join_final_output
    &gt;&gt;&gt; from langstream.contrib import OpenAIChatStream, OpenAIChatMessage, OpenAIChatDelta
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_stream: Stream[str, str] = OpenAIChatStream[str, OpenAIChatDelta](
    ...         &#34;RecipeStream&#34;,
    ...         lambda recipe_name: [
    ...             OpenAIChatMessage(
    ...                 role=&#34;system&#34;,
    ...                 content=&#34;You are ChefGPT, an assistant bot trained on all culinary knowledge of world&#39;s most proeminant Michelin Chefs&#34;,
    ...             ),
    ...             OpenAIChatMessage(
    ...                 role=&#34;user&#34;,
    ...                 content=f&#34;Hello, could you write me a recipe for {recipe_name}?&#34;,
    ...             ),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         max_tokens=10,
    ...     ).map(lambda delta: delta.content)
    ...
    ...     return await join_final_output(recipe_stream(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#34;Of course! Here&#39;s a simple and delicious recipe&#34;

    You can also pass OpenAI function schemas in the `function` argument with all parameter definitions, the model may then produce a `function` role `OpenAIChatDelta`,
    using your function, with the `content` field as a json which you can parse to call an actual function.

    Take a look [at our guide](https://rogeriochaves.github.io/langstream/docs/llms/open_ai_functions) to learn more about OpenAI function calls in LangStream.

    Function Call Example
    ---------------------

    &gt;&gt;&gt; from langstream import Stream, collect_final_output
    &gt;&gt;&gt; from langstream.contrib import OpenAIChatStream, OpenAIChatMessage, OpenAIChatDelta
    &gt;&gt;&gt; from typing import Literal, Union, Dict
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     def get_current_weather(
    ...         location: str, format: Literal[&#34;celsius&#34;, &#34;fahrenheit&#34;] = &#34;celsius&#34;
    ...     ) -&gt; Dict[str, str]:
    ...         return {
    ...             &#34;location&#34;: location,
    ...             &#34;forecast&#34;: &#34;sunny&#34;,
    ...             &#34;temperature&#34;: &#34;25 C&#34; if format == &#34;celsius&#34; else &#34;77 F&#34;,
    ...         }
    ...
    ...     stream : Stream[str, Union[OpenAIChatDelta, Dict[str, str]]] = OpenAIChatStream[str, Union[OpenAIChatDelta, Dict[str, str]]](
    ...         &#34;WeatherStream&#34;,
    ...         lambda user_input: [
    ...             OpenAIChatMessage(role=&#34;user&#34;, content=user_input),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         functions=[
    ...             {
    ...                 &#34;name&#34;: &#34;get_current_weather&#34;,
    ...                 &#34;description&#34;: &#34;Gets the current weather in a given location, use this function for any questions related to the weather&#34;,
    ...                 &#34;parameters&#34;: {
    ...                     &#34;type&#34;: &#34;object&#34;,
    ...                     &#34;properties&#34;: {
    ...                         &#34;location&#34;: {
    ...                             &#34;description&#34;: &#34;The city to get the weather, e.g. San Francisco. Guess the location from user messages&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                         },
    ...                         &#34;format&#34;: {
    ...                             &#34;description&#34;: &#34;A string with the full content of what the given role said&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                             &#34;enum&#34;: (&#34;celsius&#34;, &#34;fahrenheit&#34;),
    ...                         },
    ...                     },
    ...                     &#34;required&#34;: [&#34;location&#34;],
    ...                 },
    ...             }
    ...         ],
    ...         temperature=0,
    ...     ).map(
    ...         lambda delta: get_current_weather(**json.loads(delta.content))
    ...         if delta.role == &#34;function&#34; and delta.name == &#34;get_current_weather&#34;
    ...         else delta
    ...     )
    ...
    ...     return await collect_final_output(stream(&#34;how is the weather today in Rio de Janeiro?&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    [{&#39;location&#39;: &#39;Rio de Janeiro&#39;, &#39;forecast&#39;: &#39;sunny&#39;, &#39;temperature&#39;: &#39;25 C&#39;}]

    &#34;&#34;&#34;

    def __init__(
        self: &#34;OpenAIChatStream[T, OpenAIChatDelta]&#34;,
        name: str,
        call: Callable[
            [T],
            List[OpenAIChatMessage],
        ],
        model: str,
        functions: Optional[List[Dict[str, Any]]] = None,
        function_call: Optional[Union[Literal[&#34;none&#34;, &#34;auto&#34;], Dict[str, Any]]] = None,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
        timeout: int = 5,
        retries: int = 3,
    ) -&gt; None:
        async def chat_completion(
            messages: List[OpenAIChatMessage],
        ) -&gt; AsyncGenerator[StreamOutput[OpenAIChatDelta], None]:
            loop = asyncio.get_event_loop()

            @retry(tries=retries)
            def get_completions():
                function_kwargs = {}
                if functions is not None:
                    function_kwargs[&#34;functions&#34;] = functions
                if function_call is not None:
                    function_kwargs[&#34;function_call&#34;] = function_call

                openai = importlib.import_module(&#34;openai&#34;)
                # import openai

                client = openai.OpenAI()
                return client.chat.completions.create(
                    timeout=timeout,
                    model=model,
                    messages=cast(Any, [m.to_dict() for m in messages]),
                    temperature=temperature,
                    stream=True,
                    max_tokens=max_tokens,
                    **function_kwargs,
                )

            completions = await loop.run_in_executor(None, get_completions)

            pending_function_call: Optional[OpenAIChatDelta] = None

            for output in completions:
                if len(output.choices) == 0:
                    continue

                delta = output.choices[0].delta
                if not delta:
                    continue

                if delta.function_call is not None:
                    role = delta.role
                    function_name: Optional[str] = delta.function_call.name
                    function_arguments: Optional[str] = delta.function_call.arguments

                    if function_name is not None:
                        pending_function_call = OpenAIChatDelta(
                            role=&#34;function&#34;,
                            name=function_name,
                            content=function_arguments or &#34;&#34;,
                        )
                    elif (
                        pending_function_call is not None
                        and function_arguments is not None
                    ):
                        pending_function_call.content += function_arguments
                elif delta.content is not None:
                    role = cast(
                        Union[Literal[&#34;assistant&#34;, &#34;function&#34;], None], delta.role
                    )
                    yield self._output_wrap(
                        OpenAIChatDelta(
                            role=role,
                            content=delta.content,
                        )
                    )
                else:
                    if pending_function_call:
                        yield self._output_wrap(pending_function_call)
                        pending_function_call = None
            if pending_function_call:
                yield self._output_wrap(pending_function_call)
                pending_function_call = None

        super().__init__(
            name,
            lambda input: cast(AsyncGenerator[U, None], chat_completion(call(input))),
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatDelta"><code class="flex name class">
<span>class <span class="ident">OpenAIChatDelta</span></span>
<span>(</span><span>role:¬†Optional[Literal['assistant',¬†'function']], content:¬†str, name:¬†Optional[str]¬†=¬†None)</span>
</code></dt>
<dd>
<div class="desc"><p>OpenAIChatDelta is a data class that represents the output of an <code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code>.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>Optional[Literal["assistant", "function"]]</code></dt>
<dd>The role of the output message, the first message will have the role, while
the subsequent partial content output ones will have the role as <code>None</code>.
For now the only possible values it will have is either None or <code>"assistant"</code></dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>The name is used for when <code>role</code> is <code>"function"</code>, it represents the name of the function that was called</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the partial content being outputted by the LLM, this generally
translate to each token the LLM is producing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class OpenAIChatDelta:
    &#34;&#34;&#34;
    OpenAIChatDelta is a data class that represents the output of an `OpenAIChatStream`.

    Attributes
    ----------
    role : Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
        The role of the output message, the first message will have the role, while
        the subsequent partial content output ones will have the role as `None`.
        For now the only possible values it will have is either None or `&#34;assistant&#34;`

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the partial content being outputted by the LLM, this generally
        translate to each token the LLM is producing

    &#34;&#34;&#34;

    role: Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
    content: str
    name: Optional[str] = None

    def __stream_debug__(self):
        name = &#34;&#34;
        if self.name:
            name = f&#34; {self.name}&#34;
        if self.role is not None:
            print(f&#34;{Fore.YELLOW}{self.role.capitalize()}{name}:{Fore.RESET} &#34;, end=&#34;&#34;)
        print(
            self.content,
            end=&#34;&#34;,
            flush=True,
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatDelta.content"><code class="name">var <span class="ident">content</span> :¬†str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatDelta.name"><code class="name">var <span class="ident">name</span> :¬†Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatDelta.role"><code class="name">var <span class="ident">role</span> :¬†Optional[Literal['assistant',¬†'function']]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatMessage"><code class="flex name class">
<span>class <span class="ident">OpenAIChatMessage</span></span>
<span>(</span><span>role:¬†Literal['system',¬†'user',¬†'assistant',¬†'function'], content:¬†str, name:¬†Optional[str]¬†=¬†None)</span>
</code></dt>
<dd>
<div class="desc"><p>OpenAIChatMessage is a data class that represents a chat message for building <code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code> prompt.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>Literal["system", "user", "assistant", "function"]</code></dt>
<dd>The role of who sent this message in the chat, can be one of <code>"system"</code>, <code>"user"</code>, <code>"assistant"</code> or "function"</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>The name is used for when <code>role</code> is <code>"function"</code>, it represents the name of the function that was called</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the full content of what the given role said</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class OpenAIChatMessage:
    &#34;&#34;&#34;
    OpenAIChatMessage is a data class that represents a chat message for building `OpenAIChatStream` prompt.

    Attributes
    ----------
    role : Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
        The role of who sent this message in the chat, can be one of `&#34;system&#34;`, `&#34;user&#34;`, `&#34;assistant&#34;` or &#34;function&#34;

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the full content of what the given role said

    &#34;&#34;&#34;

    role: Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
    content: str
    name: Optional[str] = None

    def to_dict(self):
        return {k: v for k, v in self.__dict__.items() if v is not None}</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatMessage.content"><code class="name">var <span class="ident">content</span> :¬†str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatMessage.name"><code class="name">var <span class="ident">name</span> :¬†Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatMessage.role"><code class="name">var <span class="ident">role</span> :¬†Literal['system',¬†'user',¬†'assistant',¬†'function']</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatMessage.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self):
    return {k: v for k, v in self.__dict__.items() if v is not None}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAIChatStream"><code class="flex name class">
<span>class <span class="ident">OpenAIChatStream</span></span>
<span>(</span><span>name:¬†str, call:¬†Callable[[~T],¬†List[<a title="langstream.contrib.llms.open_ai.OpenAIChatMessage" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage">OpenAIChatMessage</a>]], model:¬†str, functions:¬†Optional[List[Dict[str,¬†Any]]]¬†=¬†None, function_call:¬†Union[Literal['none',¬†'auto'],¬†Dict[str,¬†Any],¬†ForwardRef(None)]¬†=¬†None, temperature:¬†Optional[float]¬†=¬†0, max_tokens:¬†Optional[int]¬†=¬†None, timeout:¬†int¬†=¬†5, retries:¬†int¬†=¬†3)</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code> gives you access to the more powerful LLMs from OpenAI, like <code>gpt-3.5-turbo</code> and <code>gpt-4</code>, they are structured in a chat format with roles.</p>
<p>The <code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code> takes a lambda function that should return a list of <code><a title="langstream.contrib.llms.open_ai.OpenAIChatMessage" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage">OpenAIChatMessage</a></code> for the assistant to reply, it is stateless, so it doesn't keep
memory of the past chat messages, you will have to handle the memory yourself, you can <a href="https://rogeriochaves.github.io/langstream/docs/llms/memory">follow this guide to get started on memory</a>.</p>
<p>The <code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code> also produces <code><a title="langstream.contrib.llms.open_ai.OpenAIChatDelta" href="#langstream.contrib.llms.open_ai.OpenAIChatDelta">OpenAIChatDelta</a></code> as output, one per token, it contains the <code>role</code> that started the output, and then subsequent <code>content</code> updates.
If you want the final content as a string, you will need to use the <code>.content</code> property from the delta and accumulate it for the final result.</p>
<p>To use this stream you will need an <code>OPENAI_API_KEY</code> environment variable to be available, and then you can generate chat completions out of it.</p>
<p>You can read more about the chat completion API on <a href="https://platform.openai.com/docs/api-reference/chat">OpenAI API reference</a></p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from langstream import Stream, join_final_output
&gt;&gt;&gt; from langstream.contrib import OpenAIChatStream, OpenAIChatMessage, OpenAIChatDelta
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     recipe_stream: Stream[str, str] = OpenAIChatStream[str, OpenAIChatDelta](
...         &quot;RecipeStream&quot;,
...         lambda recipe_name: [
...             OpenAIChatMessage(
...                 role=&quot;system&quot;,
...                 content=&quot;You are ChefGPT, an assistant bot trained on all culinary knowledge of world's most proeminant Michelin Chefs&quot;,
...             ),
...             OpenAIChatMessage(
...                 role=&quot;user&quot;,
...                 content=f&quot;Hello, could you write me a recipe for {recipe_name}?&quot;,
...             ),
...         ],
...         model=&quot;gpt-3.5-turbo&quot;,
...         max_tokens=10,
...     ).map(lambda delta: delta.content)
...
...     return await join_final_output(recipe_stream(&quot;instant noodles&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
&quot;Of course! Here's a simple and delicious recipe&quot;
</code></pre>
<p>You can also pass OpenAI function schemas in the <code>function</code> argument with all parameter definitions, the model may then produce a <code>function</code> role <code><a title="langstream.contrib.llms.open_ai.OpenAIChatDelta" href="#langstream.contrib.llms.open_ai.OpenAIChatDelta">OpenAIChatDelta</a></code>,
using your function, with the <code>content</code> field as a json which you can parse to call an actual function.</p>
<p>Take a look <a href="https://rogeriochaves.github.io/langstream/docs/llms/open_ai_functions">at our guide</a> to learn more about OpenAI function calls in LangStream.</p>
<h2 id="function-call-example">Function Call Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from langstream import Stream, collect_final_output
&gt;&gt;&gt; from langstream.contrib import OpenAIChatStream, OpenAIChatMessage, OpenAIChatDelta
&gt;&gt;&gt; from typing import Literal, Union, Dict
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     def get_current_weather(
...         location: str, format: Literal[&quot;celsius&quot;, &quot;fahrenheit&quot;] = &quot;celsius&quot;
...     ) -&gt; Dict[str, str]:
...         return {
...             &quot;location&quot;: location,
...             &quot;forecast&quot;: &quot;sunny&quot;,
...             &quot;temperature&quot;: &quot;25 C&quot; if format == &quot;celsius&quot; else &quot;77 F&quot;,
...         }
...
...     stream : Stream[str, Union[OpenAIChatDelta, Dict[str, str]]] = OpenAIChatStream[str, Union[OpenAIChatDelta, Dict[str, str]]](
...         &quot;WeatherStream&quot;,
...         lambda user_input: [
...             OpenAIChatMessage(role=&quot;user&quot;, content=user_input),
...         ],
...         model=&quot;gpt-3.5-turbo&quot;,
...         functions=[
...             {
...                 &quot;name&quot;: &quot;get_current_weather&quot;,
...                 &quot;description&quot;: &quot;Gets the current weather in a given location, use this function for any questions related to the weather&quot;,
...                 &quot;parameters&quot;: {
...                     &quot;type&quot;: &quot;object&quot;,
...                     &quot;properties&quot;: {
...                         &quot;location&quot;: {
...                             &quot;description&quot;: &quot;The city to get the weather, e.g. San Francisco. Guess the location from user messages&quot;,
...                             &quot;type&quot;: &quot;string&quot;,
...                         },
...                         &quot;format&quot;: {
...                             &quot;description&quot;: &quot;A string with the full content of what the given role said&quot;,
...                             &quot;type&quot;: &quot;string&quot;,
...                             &quot;enum&quot;: (&quot;celsius&quot;, &quot;fahrenheit&quot;),
...                         },
...                     },
...                     &quot;required&quot;: [&quot;location&quot;],
...                 },
...             }
...         ],
...         temperature=0,
...     ).map(
...         lambda delta: get_current_weather(**json.loads(delta.content))
...         if delta.role == &quot;function&quot; and delta.name == &quot;get_current_weather&quot;
...         else delta
...     )
...
...     return await collect_final_output(stream(&quot;how is the weather today in Rio de Janeiro?&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
[{'location': 'Rio de Janeiro', 'forecast': 'sunny', 'temperature': '25 C'}]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OpenAIChatStream(Stream[T, U]):
    &#34;&#34;&#34;
    `OpenAIChatStream` gives you access to the more powerful LLMs from OpenAI, like `gpt-3.5-turbo` and `gpt-4`, they are structured in a chat format with roles.

    The `OpenAIChatStream` takes a lambda function that should return a list of `OpenAIChatMessage` for the assistant to reply, it is stateless, so it doesn&#39;t keep
    memory of the past chat messages, you will have to handle the memory yourself, you can [follow this guide to get started on memory](https://rogeriochaves.github.io/langstream/docs/llms/memory).

    The `OpenAIChatStream` also produces `OpenAIChatDelta` as output, one per token, it contains the `role` that started the output, and then subsequent `content` updates.
    If you want the final content as a string, you will need to use the `.content` property from the delta and accumulate it for the final result.

    To use this stream you will need an `OPENAI_API_KEY` environment variable to be available, and then you can generate chat completions out of it.

    You can read more about the chat completion API on [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat)

    Example
    -------

    &gt;&gt;&gt; from langstream import Stream, join_final_output
    &gt;&gt;&gt; from langstream.contrib import OpenAIChatStream, OpenAIChatMessage, OpenAIChatDelta
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_stream: Stream[str, str] = OpenAIChatStream[str, OpenAIChatDelta](
    ...         &#34;RecipeStream&#34;,
    ...         lambda recipe_name: [
    ...             OpenAIChatMessage(
    ...                 role=&#34;system&#34;,
    ...                 content=&#34;You are ChefGPT, an assistant bot trained on all culinary knowledge of world&#39;s most proeminant Michelin Chefs&#34;,
    ...             ),
    ...             OpenAIChatMessage(
    ...                 role=&#34;user&#34;,
    ...                 content=f&#34;Hello, could you write me a recipe for {recipe_name}?&#34;,
    ...             ),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         max_tokens=10,
    ...     ).map(lambda delta: delta.content)
    ...
    ...     return await join_final_output(recipe_stream(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#34;Of course! Here&#39;s a simple and delicious recipe&#34;

    You can also pass OpenAI function schemas in the `function` argument with all parameter definitions, the model may then produce a `function` role `OpenAIChatDelta`,
    using your function, with the `content` field as a json which you can parse to call an actual function.

    Take a look [at our guide](https://rogeriochaves.github.io/langstream/docs/llms/open_ai_functions) to learn more about OpenAI function calls in LangStream.

    Function Call Example
    ---------------------

    &gt;&gt;&gt; from langstream import Stream, collect_final_output
    &gt;&gt;&gt; from langstream.contrib import OpenAIChatStream, OpenAIChatMessage, OpenAIChatDelta
    &gt;&gt;&gt; from typing import Literal, Union, Dict
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     def get_current_weather(
    ...         location: str, format: Literal[&#34;celsius&#34;, &#34;fahrenheit&#34;] = &#34;celsius&#34;
    ...     ) -&gt; Dict[str, str]:
    ...         return {
    ...             &#34;location&#34;: location,
    ...             &#34;forecast&#34;: &#34;sunny&#34;,
    ...             &#34;temperature&#34;: &#34;25 C&#34; if format == &#34;celsius&#34; else &#34;77 F&#34;,
    ...         }
    ...
    ...     stream : Stream[str, Union[OpenAIChatDelta, Dict[str, str]]] = OpenAIChatStream[str, Union[OpenAIChatDelta, Dict[str, str]]](
    ...         &#34;WeatherStream&#34;,
    ...         lambda user_input: [
    ...             OpenAIChatMessage(role=&#34;user&#34;, content=user_input),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         functions=[
    ...             {
    ...                 &#34;name&#34;: &#34;get_current_weather&#34;,
    ...                 &#34;description&#34;: &#34;Gets the current weather in a given location, use this function for any questions related to the weather&#34;,
    ...                 &#34;parameters&#34;: {
    ...                     &#34;type&#34;: &#34;object&#34;,
    ...                     &#34;properties&#34;: {
    ...                         &#34;location&#34;: {
    ...                             &#34;description&#34;: &#34;The city to get the weather, e.g. San Francisco. Guess the location from user messages&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                         },
    ...                         &#34;format&#34;: {
    ...                             &#34;description&#34;: &#34;A string with the full content of what the given role said&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                             &#34;enum&#34;: (&#34;celsius&#34;, &#34;fahrenheit&#34;),
    ...                         },
    ...                     },
    ...                     &#34;required&#34;: [&#34;location&#34;],
    ...                 },
    ...             }
    ...         ],
    ...         temperature=0,
    ...     ).map(
    ...         lambda delta: get_current_weather(**json.loads(delta.content))
    ...         if delta.role == &#34;function&#34; and delta.name == &#34;get_current_weather&#34;
    ...         else delta
    ...     )
    ...
    ...     return await collect_final_output(stream(&#34;how is the weather today in Rio de Janeiro?&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    [{&#39;location&#39;: &#39;Rio de Janeiro&#39;, &#39;forecast&#39;: &#39;sunny&#39;, &#39;temperature&#39;: &#39;25 C&#39;}]

    &#34;&#34;&#34;

    def __init__(
        self: &#34;OpenAIChatStream[T, OpenAIChatDelta]&#34;,
        name: str,
        call: Callable[
            [T],
            List[OpenAIChatMessage],
        ],
        model: str,
        functions: Optional[List[Dict[str, Any]]] = None,
        function_call: Optional[Union[Literal[&#34;none&#34;, &#34;auto&#34;], Dict[str, Any]]] = None,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
        timeout: int = 5,
        retries: int = 3,
    ) -&gt; None:
        async def chat_completion(
            messages: List[OpenAIChatMessage],
        ) -&gt; AsyncGenerator[StreamOutput[OpenAIChatDelta], None]:
            loop = asyncio.get_event_loop()

            @retry(tries=retries)
            def get_completions():
                function_kwargs = {}
                if functions is not None:
                    function_kwargs[&#34;functions&#34;] = functions
                if function_call is not None:
                    function_kwargs[&#34;function_call&#34;] = function_call

                openai = importlib.import_module(&#34;openai&#34;)
                # import openai

                client = openai.OpenAI()
                return client.chat.completions.create(
                    timeout=timeout,
                    model=model,
                    messages=cast(Any, [m.to_dict() for m in messages]),
                    temperature=temperature,
                    stream=True,
                    max_tokens=max_tokens,
                    **function_kwargs,
                )

            completions = await loop.run_in_executor(None, get_completions)

            pending_function_call: Optional[OpenAIChatDelta] = None

            for output in completions:
                if len(output.choices) == 0:
                    continue

                delta = output.choices[0].delta
                if not delta:
                    continue

                if delta.function_call is not None:
                    role = delta.role
                    function_name: Optional[str] = delta.function_call.name
                    function_arguments: Optional[str] = delta.function_call.arguments

                    if function_name is not None:
                        pending_function_call = OpenAIChatDelta(
                            role=&#34;function&#34;,
                            name=function_name,
                            content=function_arguments or &#34;&#34;,
                        )
                    elif (
                        pending_function_call is not None
                        and function_arguments is not None
                    ):
                        pending_function_call.content += function_arguments
                elif delta.content is not None:
                    role = cast(
                        Union[Literal[&#34;assistant&#34;, &#34;function&#34;], None], delta.role
                    )
                    yield self._output_wrap(
                        OpenAIChatDelta(
                            role=role,
                            content=delta.content,
                        )
                    )
                else:
                    if pending_function_call:
                        yield self._output_wrap(pending_function_call)
                        pending_function_call = None
            if pending_function_call:
                yield self._output_wrap(pending_function_call)
                pending_function_call = None

        super().__init__(
            name,
            lambda input: cast(AsyncGenerator[U, None], chat_completion(call(input))),
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="langstream.core.stream.Stream" href="../../core/stream.html#langstream.core.stream.Stream">Stream</a></li>
<li>typing.Generic</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="langstream.core.stream.Stream" href="../../core/stream.html#langstream.core.stream.Stream">Stream</a></b></code>:
<ul class="hlist">
<li><code><a title="langstream.core.stream.Stream.and_then" href="../../core/stream.html#langstream.core.stream.Stream.and_then">and_then</a></code></li>
<li><code><a title="langstream.core.stream.Stream.collect" href="../../core/stream.html#langstream.core.stream.Stream.collect">collect</a></code></li>
<li><code><a title="langstream.core.stream.Stream.filter" href="../../core/stream.html#langstream.core.stream.Stream.filter">filter</a></code></li>
<li><code><a title="langstream.core.stream.Stream.gather" href="../../core/stream.html#langstream.core.stream.Stream.gather">gather</a></code></li>
<li><code><a title="langstream.core.stream.Stream.join" href="../../core/stream.html#langstream.core.stream.Stream.join">join</a></code></li>
<li><code><a title="langstream.core.stream.Stream.map" href="../../core/stream.html#langstream.core.stream.Stream.map">map</a></code></li>
<li><code><a title="langstream.core.stream.Stream.on_error" href="../../core/stream.html#langstream.core.stream.Stream.on_error">on_error</a></code></li>
<li><code><a title="langstream.core.stream.Stream.pipe" href="../../core/stream.html#langstream.core.stream.Stream.pipe">pipe</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="langstream.contrib.llms.open_ai.OpenAICompletionStream"><code class="flex name class">
<span>class <span class="ident">OpenAICompletionStream</span></span>
<span>(</span><span>name:¬†str, call:¬†Callable[[~T],¬†str], model:¬†str, temperature:¬†Optional[float]¬†=¬†0, max_tokens:¬†Optional[int]¬†=¬†None, timeout:¬†int¬†=¬†5, retries:¬†int¬†=¬†3)</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="langstream.contrib.llms.open_ai.OpenAICompletionStream" href="#langstream.contrib.llms.open_ai.OpenAICompletionStream">OpenAICompletionStream</a></code> uses the most simple LLMs from OpenAI based on GPT-3 for text completion, if you are looking for ChatCompletion, take a look at <code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code>.</p>
<p>The <code><a title="langstream.contrib.llms.open_ai.OpenAICompletionStream" href="#langstream.contrib.llms.open_ai.OpenAICompletionStream">OpenAICompletionStream</a></code> takes a lambda function that should return a string with the prompt for completion.</p>
<p>To use this stream you will need an <code>OPENAI_API_KEY</code> environment variable to be available, and then you can generate completions out of it.</p>
<p>You can read more about the completion API on <a href="https://platform.openai.com/docs/api-reference/completions">OpenAI API reference</a></p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from langstream import join_final_output
&gt;&gt;&gt; from langstream.contrib import OpenAICompletionStream
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     recipe_stream = OpenAICompletionStream[str, str](
...         &quot;RecipeStream&quot;,
...         lambda recipe_name: f&quot;Here is my {recipe_name} recipe: &quot;,
...         model=&quot;ada&quot;,
...     )
...
...     return await join_final_output(recipe_stream(&quot;instant noodles&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
'„ÄêInstant Noodles„Äë\n\nIngredients:\n\n1 cup of water'
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OpenAICompletionStream(Stream[T, U]):
    &#34;&#34;&#34;
    `OpenAICompletionStream` uses the most simple LLMs from OpenAI based on GPT-3 for text completion, if you are looking for ChatCompletion, take a look at `OpenAIChatStream`.

    The `OpenAICompletionStream` takes a lambda function that should return a string with the prompt for completion.

    To use this stream you will need an `OPENAI_API_KEY` environment variable to be available, and then you can generate completions out of it.

    You can read more about the completion API on [OpenAI API reference](https://platform.openai.com/docs/api-reference/completions)

    Example
    -------

    &gt;&gt;&gt; from langstream import join_final_output
    &gt;&gt;&gt; from langstream.contrib import OpenAICompletionStream
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_stream = OpenAICompletionStream[str, str](
    ...         &#34;RecipeStream&#34;,
    ...         lambda recipe_name: f&#34;Here is my {recipe_name} recipe: &#34;,
    ...         model=&#34;ada&#34;,
    ...     )
    ...
    ...     return await join_final_output(recipe_stream(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#39;„ÄêInstant Noodles„Äë\\n\\nIngredients:\\n\\n1 cup of water&#39;

    &#34;&#34;&#34;

    def __init__(
        self: &#34;OpenAICompletionStream[T, str]&#34;,
        name: str,
        call: Callable[
            [T],
            str,
        ],
        model: str,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
        timeout: int = 5,
        retries: int = 3,
    ) -&gt; None:
        async def completion(prompt: str) -&gt; AsyncGenerator[U, None]:
            loop = asyncio.get_event_loop()

            @retry(tries=retries)
            def get_completions():
                openai = importlib.import_module(&#34;openai&#34;)
                client = openai.OpenAI()
                return client.completions.create(
                    model=model,
                    prompt=prompt,
                    temperature=temperature,
                    stream=True,
                    max_tokens=max_tokens,
                    timeout=timeout,
                )

            completions = await loop.run_in_executor(None, get_completions)

            for output in completions:
                output = cast(dict, output.model_dump())
                if &#34;choices&#34; in output:
                    if len(output[&#34;choices&#34;]) &gt; 0:
                        if &#34;text&#34; in output[&#34;choices&#34;][0]:
                            yield output[&#34;choices&#34;][0][&#34;text&#34;]

        super().__init__(name, lambda input: completion(call(input)))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="langstream.core.stream.Stream" href="../../core/stream.html#langstream.core.stream.Stream">Stream</a></li>
<li>typing.Generic</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="langstream.core.stream.Stream" href="../../core/stream.html#langstream.core.stream.Stream">Stream</a></b></code>:
<ul class="hlist">
<li><code><a title="langstream.core.stream.Stream.and_then" href="../../core/stream.html#langstream.core.stream.Stream.and_then">and_then</a></code></li>
<li><code><a title="langstream.core.stream.Stream.collect" href="../../core/stream.html#langstream.core.stream.Stream.collect">collect</a></code></li>
<li><code><a title="langstream.core.stream.Stream.filter" href="../../core/stream.html#langstream.core.stream.Stream.filter">filter</a></code></li>
<li><code><a title="langstream.core.stream.Stream.gather" href="../../core/stream.html#langstream.core.stream.Stream.gather">gather</a></code></li>
<li><code><a title="langstream.core.stream.Stream.join" href="../../core/stream.html#langstream.core.stream.Stream.join">join</a></code></li>
<li><code><a title="langstream.core.stream.Stream.map" href="../../core/stream.html#langstream.core.stream.Stream.map">map</a></code></li>
<li><code><a title="langstream.core.stream.Stream.on_error" href="../../core/stream.html#langstream.core.stream.Stream.on_error">on_error</a></code></li>
<li><code><a title="langstream.core.stream.Stream.pipe" href="../../core/stream.html#langstream.core.stream.Stream.pipe">pipe</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/langstream/docs/intro" style="color: #000">‚Üê Back to Docs</a>
<h1>ü™Ωüîó LangStream API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="langstream.contrib.llms" href="index.html">langstream.contrib.llms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="langstream.contrib.llms.open_ai.OpenAIChatDelta" href="#langstream.contrib.llms.open_ai.OpenAIChatDelta">OpenAIChatDelta</a></code></h4>
<ul class="">
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatDelta.content" href="#langstream.contrib.llms.open_ai.OpenAIChatDelta.content">content</a></code></li>
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatDelta.name" href="#langstream.contrib.llms.open_ai.OpenAIChatDelta.name">name</a></code></li>
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatDelta.role" href="#langstream.contrib.llms.open_ai.OpenAIChatDelta.role">role</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langstream.contrib.llms.open_ai.OpenAIChatMessage" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage">OpenAIChatMessage</a></code></h4>
<ul class="">
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatMessage.content" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage.content">content</a></code></li>
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatMessage.name" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage.name">name</a></code></li>
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatMessage.role" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage.role">role</a></code></li>
<li><code><a title="langstream.contrib.llms.open_ai.OpenAIChatMessage.to_dict" href="#langstream.contrib.llms.open_ai.OpenAIChatMessage.to_dict">to_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langstream.contrib.llms.open_ai.OpenAIChatStream" href="#langstream.contrib.llms.open_ai.OpenAIChatStream">OpenAIChatStream</a></code></h4>
</li>
<li>
<h4><code><a title="langstream.contrib.llms.open_ai.OpenAICompletionStream" href="#langstream.contrib.llms.open_ai.OpenAICompletionStream">OpenAICompletionStream</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>