<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>langstream.contrib.llms.lite_llm API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü™Ω</text></svg>" data-rh="true">
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/langstream/"
><b class="navbar__title text--truncate">ü™Ωüîó LangStream</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/langstream/docs/intro"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/langstream/reference/langstream/index.html"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/rogeriochaves/langstream"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>langstream.contrib.llms.lite_llm</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import asyncio
from dataclasses import dataclass
from types import GeneratorType
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    TypeVar,
    Union,
    cast,
)

import importlib
from colorama import Fore
from retry import retry

from langstream.core.stream import Stream, StreamOutput

T = TypeVar(&#34;T&#34;)
U = TypeVar(&#34;U&#34;)
V = TypeVar(&#34;V&#34;)


@dataclass
class LiteLLMChatMessage:
    &#34;&#34;&#34;
    LiteLLMChatMessage is a data class that represents a chat message for building `LiteLLMChatStream` prompt.

    Attributes
    ----------
    role : Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
        The role of who sent this message in the chat, can be one of `&#34;system&#34;`, `&#34;user&#34;`, `&#34;assistant&#34;` or &#34;function&#34;

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the full content of what the given role said

    &#34;&#34;&#34;

    role: Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
    content: str
    name: Optional[str] = None

    def to_dict(self):
        return {k: v for k, v in self.__dict__.items() if v is not None}


@dataclass
class LiteLLMChatDelta:
    &#34;&#34;&#34;
    LiteLLMChatDelta is a data class that represents the output of an `LiteLLMChatStream`.

    Attributes
    ----------
    role : Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
        The role of the output message, the first message will have the role, while
        the subsequent partial content output ones will have the role as `None`.
        For now the only possible values it will have is either None or `&#34;assistant&#34;`

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the partial content being outputted by the LLM, this generally
        translate to each token the LLM is producing

    &#34;&#34;&#34;

    role: Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
    content: str
    name: Optional[str] = None

    def __stream_debug__(self):
        name = &#34;&#34;
        if self.name:
            name = f&#34; {self.name}&#34;
        if self.role is not None:
            print(f&#34;{Fore.YELLOW}{self.role.capitalize()}{name}:{Fore.RESET} &#34;, end=&#34;&#34;)
        print(
            self.content,
            end=&#34;&#34;,
            flush=True,
        )


class LiteLLMChatStream(Stream[T, U]):
    &#34;&#34;&#34;
    `LiteLLMChatStream` is a wrapper for [LiteLLM](https://github.com/BerriAI/litellm), which gives you access to OpenAI, Azure OpenAI, Anthropic, Google VertexAI,
    HuggingFace, Replicate, A21, Cohere and a bunch other LLMs all the the same time, all while keeping the standard OpenAI chat interface. Check it out the completion API
    and the available models [on their docs](https://docs.litellm.ai/docs/).

    The `LiteLLMChatStream` takes a lambda function that should return a list of `LiteLLMChatMessage` for the assistant to reply, it is stateless, so it doesn&#39;t keep
    memory of the past chat messages, you will have to handle the memory yourself, you can [follow this guide to get started on memory](https://rogeriochaves.github.io/langstream/docs/llms/memory).

    The `LiteLLMChatStream` also produces `LiteLLMChatDelta` as output, one per token, it contains the `role` that started the output, and then subsequent `content` updates.
    If you want the final content as a string, you will need to use the `.content` property from the delta and accumulate it for the final result.

    To use this stream you will need to have the proper environment keys available depending on the model you are using, like `OPENAI_API_KEY`, `COHERE_API_KEY`, `HUGGINGFACE_API_KEY`, etc,
    check it out more details on [LiteLLM docs](https://docs.litellm.ai/docs/completion/supported)

    Example
    -------

    &gt;&gt;&gt; from langstream import Stream, join_final_output
    &gt;&gt;&gt; from langstream.contrib import LiteLLMChatStream, LiteLLMChatMessage, LiteLLMChatDelta
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_stream: Stream[str, str] = LiteLLMChatStream[str, LiteLLMChatDelta](
    ...         &#34;RecipeStream&#34;,
    ...         lambda recipe_name: [
    ...             LiteLLMChatMessage(
    ...                 role=&#34;system&#34;,
    ...                 content=&#34;You are Chef Claude, an assistant bot trained on all culinary knowledge of world&#39;s most proeminant Michelin Chefs&#34;,
    ...             ),
    ...             LiteLLMChatMessage(
    ...                 role=&#34;user&#34;,
    ...                 content=f&#34;Hello, could you write me a recipe for {recipe_name}?&#34;,
    ...             ),
    ...         ],
    ...         model=&#34;claude-2&#34;,
    ...         max_tokens=10,
    ...     ).map(lambda delta: delta.content)
    ...
    ...     return await join_final_output(recipe_stream(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#34;Of course! Here&#39;s a simple and delicious recipe&#34;

    You can also pass LiteLLM function schemas in the `function` argument with all parameter definitions, just like for OpenAI model, but be aware that not all models support it.
    Once you pass a `function` param, the model may then produce a `function` role `LiteLLMChatDelta` as output,
    using your function, with the `content` field as a json which you can parse to call an actual function.

    Take a look [at our OpenAI guide](https://rogeriochaves.github.io/langstream/docs/llms/open_ai_functions) to learn more about LLM function calls in LangStream, it works the same with LiteLLM.

    Function Call Example
    ---------------------

    &gt;&gt;&gt; from langstream import Stream, collect_final_output
    &gt;&gt;&gt; from langstream.contrib import LiteLLMChatStream, LiteLLMChatMessage, LiteLLMChatDelta
    &gt;&gt;&gt; from typing import Literal, Union, Dict
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     def get_current_weather(
    ...         location: str, format: Literal[&#34;celsius&#34;, &#34;fahrenheit&#34;] = &#34;celsius&#34;
    ...     ) -&gt; Dict[str, str]:
    ...         return {
    ...             &#34;location&#34;: location,
    ...             &#34;forecast&#34;: &#34;sunny&#34;,
    ...             &#34;temperature&#34;: &#34;25 C&#34; if format == &#34;celsius&#34; else &#34;77 F&#34;,
    ...         }
    ...
    ...     stream : Stream[str, Union[LiteLLMChatDelta, Dict[str, str]]] = LiteLLMChatStream[str, Union[LiteLLMChatDelta, Dict[str, str]]](
    ...         &#34;WeatherStream&#34;,
    ...         lambda user_input: [
    ...             LiteLLMChatMessage(role=&#34;user&#34;, content=user_input),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         functions=[
    ...             {
    ...                 &#34;name&#34;: &#34;get_current_weather&#34;,
    ...                 &#34;description&#34;: &#34;Gets the current weather in a given location, use this function for any questions related to the weather&#34;,
    ...                 &#34;parameters&#34;: {
    ...                     &#34;type&#34;: &#34;object&#34;,
    ...                     &#34;properties&#34;: {
    ...                         &#34;location&#34;: {
    ...                             &#34;description&#34;: &#34;The city to get the weather, e.g. San Francisco. Guess the location from user messages&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                         },
    ...                         &#34;format&#34;: {
    ...                             &#34;description&#34;: &#34;A string with the full content of what the given role said&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                             &#34;enum&#34;: (&#34;celsius&#34;, &#34;fahrenheit&#34;),
    ...                         },
    ...                     },
    ...                     &#34;required&#34;: [&#34;location&#34;],
    ...                 },
    ...             }
    ...         ],
    ...         temperature=0,
    ...     ).map(
    ...         lambda delta: get_current_weather(**json.loads(delta.content))
    ...         if delta.role == &#34;function&#34; and delta.name == &#34;get_current_weather&#34;
    ...         else delta
    ...     )
    ...
    ...     return await collect_final_output(stream(&#34;how is the weather today in Rio de Janeiro?&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    [{&#39;location&#39;: &#39;Rio de Janeiro&#39;, &#39;forecast&#39;: &#39;sunny&#39;, &#39;temperature&#39;: &#39;25 C&#39;}]

    &#34;&#34;&#34;

    def __init__(
        self: &#34;LiteLLMChatStream[T, LiteLLMChatDelta]&#34;,
        name: str,
        call: Callable[
            [T],
            List[LiteLLMChatMessage],
        ],
        model: str,
        custom_llm_provider: Optional[str] = None,
        functions: Optional[List[Dict[str, Any]]] = None,
        function_call: Optional[Union[Literal[&#34;none&#34;, &#34;auto&#34;], Dict[str, Any]]] = None,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
        timeout: int = 5,
        retries: int = 3,
    ) -&gt; None:
        async def chat_completion(
            messages: List[LiteLLMChatMessage],
        ) -&gt; AsyncGenerator[StreamOutput[LiteLLMChatDelta], None]:
            loop = asyncio.get_event_loop()

            @retry(tries=retries)
            def get_completions():
                function_kwargs = {}
                if functions is not None:
                    function_kwargs[&#34;functions&#34;] = functions
                if function_call is not None:
                    function_kwargs[&#34;function_call&#34;] = function_call

                litellm = importlib.import_module(&#34;litellm&#34;)
                # import litellm

                return litellm.completion(
                    request_timeout=timeout,
                    model=model,
                    custom_llm_provider=custom_llm_provider,
                    messages=[m.to_dict() for m in messages],
                    temperature=temperature,  # type: ignore (why is their type int?)
                    stream=True,
                    max_tokens=max_tokens,  # type: ignore (why is their type float?)
                    **function_kwargs,
                )

            completions = await loop.run_in_executor(None, get_completions)

            pending_function_call: Optional[LiteLLMChatDelta] = None

            completions = (
                completions if hasattr(completions, &#34;__iter__&#34;) else [completions]
            )
            # from litellm import ModelResponse
            # from litellm.utils import StreamingChoices

            for output in completions:
                # output = cast(ModelResponse, output)
                if len(output.choices) == 0:
                    continue

                # choices = cast(List[StreamingChoices], output.choices)
                choices = output.choices
                delta = choices[0].delta
                if not delta:
                    continue

                delta_function_call = delta.model_dump().get(&#34;function_call&#34;)
                if delta_function_call is not None:
                    role = delta.role
                    function_name: Optional[str] = (
                        delta_function_call[&#34;name&#34;]
                        if &#34;name&#34; in delta_function_call
                        else None
                    )
                    function_arguments: Optional[str] = (
                        delta_function_call[&#34;arguments&#34;]
                        if &#34;arguments&#34; in delta_function_call
                        else None
                    )

                    if function_name is not None:
                        pending_function_call = LiteLLMChatDelta(
                            role=&#34;function&#34;,
                            name=function_name,
                            content=function_arguments or &#34;&#34;,
                        )
                    elif (
                        pending_function_call is not None
                        and function_arguments is not None
                    ):
                        pending_function_call.content += function_arguments
                elif delta.content is not None:
                    role = cast(
                        Union[Literal[&#34;assistant&#34;, &#34;function&#34;], None], delta.role
                    )
                    yield self._output_wrap(
                        LiteLLMChatDelta(
                            role=role,
                            content=delta.content,
                        )
                    )
                else:
                    if pending_function_call:
                        yield self._output_wrap(pending_function_call)
                        pending_function_call = None
            if pending_function_call:
                yield self._output_wrap(pending_function_call)
                pending_function_call = None

        super().__init__(
            name,
            lambda input: cast(AsyncGenerator[U, None], chat_completion(call(input))),
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatDelta"><code class="flex name class">
<span>class <span class="ident">LiteLLMChatDelta</span></span>
<span>(</span><span>role:¬†Optional[Literal['assistant',¬†'function']], content:¬†str, name:¬†Optional[str]¬†=¬†None)</span>
</code></dt>
<dd>
<div class="desc"><p>LiteLLMChatDelta is a data class that represents the output of an <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatStream" href="#langstream.contrib.llms.lite_llm.LiteLLMChatStream">LiteLLMChatStream</a></code>.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>Optional[Literal["assistant", "function"]]</code></dt>
<dd>The role of the output message, the first message will have the role, while
the subsequent partial content output ones will have the role as <code>None</code>.
For now the only possible values it will have is either None or <code>"assistant"</code></dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>The name is used for when <code>role</code> is <code>"function"</code>, it represents the name of the function that was called</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the partial content being outputted by the LLM, this generally
translate to each token the LLM is producing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class LiteLLMChatDelta:
    &#34;&#34;&#34;
    LiteLLMChatDelta is a data class that represents the output of an `LiteLLMChatStream`.

    Attributes
    ----------
    role : Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
        The role of the output message, the first message will have the role, while
        the subsequent partial content output ones will have the role as `None`.
        For now the only possible values it will have is either None or `&#34;assistant&#34;`

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the partial content being outputted by the LLM, this generally
        translate to each token the LLM is producing

    &#34;&#34;&#34;

    role: Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
    content: str
    name: Optional[str] = None

    def __stream_debug__(self):
        name = &#34;&#34;
        if self.name:
            name = f&#34; {self.name}&#34;
        if self.role is not None:
            print(f&#34;{Fore.YELLOW}{self.role.capitalize()}{name}:{Fore.RESET} &#34;, end=&#34;&#34;)
        print(
            self.content,
            end=&#34;&#34;,
            flush=True,
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatDelta.content"><code class="name">var <span class="ident">content</span> :¬†str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatDelta.name"><code class="name">var <span class="ident">name</span> :¬†Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatDelta.role"><code class="name">var <span class="ident">role</span> :¬†Optional[Literal['assistant',¬†'function']]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatMessage"><code class="flex name class">
<span>class <span class="ident">LiteLLMChatMessage</span></span>
<span>(</span><span>role:¬†Literal['system',¬†'user',¬†'assistant',¬†'function'], content:¬†str, name:¬†Optional[str]¬†=¬†None)</span>
</code></dt>
<dd>
<div class="desc"><p>LiteLLMChatMessage is a data class that represents a chat message for building <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatStream" href="#langstream.contrib.llms.lite_llm.LiteLLMChatStream">LiteLLMChatStream</a></code> prompt.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>Literal["system", "user", "assistant", "function"]</code></dt>
<dd>The role of who sent this message in the chat, can be one of <code>"system"</code>, <code>"user"</code>, <code>"assistant"</code> or "function"</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>The name is used for when <code>role</code> is <code>"function"</code>, it represents the name of the function that was called</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the full content of what the given role said</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class LiteLLMChatMessage:
    &#34;&#34;&#34;
    LiteLLMChatMessage is a data class that represents a chat message for building `LiteLLMChatStream` prompt.

    Attributes
    ----------
    role : Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
        The role of who sent this message in the chat, can be one of `&#34;system&#34;`, `&#34;user&#34;`, `&#34;assistant&#34;` or &#34;function&#34;

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the full content of what the given role said

    &#34;&#34;&#34;

    role: Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
    content: str
    name: Optional[str] = None

    def to_dict(self):
        return {k: v for k, v in self.__dict__.items() if v is not None}</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.content"><code class="name">var <span class="ident">content</span> :¬†str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.name"><code class="name">var <span class="ident">name</span> :¬†Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.role"><code class="name">var <span class="ident">role</span> :¬†Literal['system',¬†'user',¬†'assistant',¬†'function']</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self):
    return {k: v for k, v in self.__dict__.items() if v is not None}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="langstream.contrib.llms.lite_llm.LiteLLMChatStream"><code class="flex name class">
<span>class <span class="ident">LiteLLMChatStream</span></span>
<span>(</span><span>name:¬†str, call:¬†Callable[[~T],¬†List[<a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage">LiteLLMChatMessage</a>]], model:¬†str, custom_llm_provider:¬†Optional[str]¬†=¬†None, functions:¬†Optional[List[Dict[str,¬†Any]]]¬†=¬†None, function_call:¬†Union[Literal['none',¬†'auto'],¬†Dict[str,¬†Any],¬†ForwardRef(None)]¬†=¬†None, temperature:¬†Optional[float]¬†=¬†0, max_tokens:¬†Optional[int]¬†=¬†None, timeout:¬†int¬†=¬†5, retries:¬†int¬†=¬†3)</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatStream" href="#langstream.contrib.llms.lite_llm.LiteLLMChatStream">LiteLLMChatStream</a></code> is a wrapper for <a href="https://github.com/BerriAI/litellm">LiteLLM</a>, which gives you access to OpenAI, Azure OpenAI, Anthropic, Google VertexAI,
HuggingFace, Replicate, A21, Cohere and a bunch other LLMs all the the same time, all while keeping the standard OpenAI chat interface. Check it out the completion API
and the available models <a href="https://docs.litellm.ai/docs/">on their docs</a>.</p>
<p>The <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatStream" href="#langstream.contrib.llms.lite_llm.LiteLLMChatStream">LiteLLMChatStream</a></code> takes a lambda function that should return a list of <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage">LiteLLMChatMessage</a></code> for the assistant to reply, it is stateless, so it doesn't keep
memory of the past chat messages, you will have to handle the memory yourself, you can <a href="https://rogeriochaves.github.io/langstream/docs/llms/memory">follow this guide to get started on memory</a>.</p>
<p>The <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatStream" href="#langstream.contrib.llms.lite_llm.LiteLLMChatStream">LiteLLMChatStream</a></code> also produces <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatDelta" href="#langstream.contrib.llms.lite_llm.LiteLLMChatDelta">LiteLLMChatDelta</a></code> as output, one per token, it contains the <code>role</code> that started the output, and then subsequent <code>content</code> updates.
If you want the final content as a string, you will need to use the <code>.content</code> property from the delta and accumulate it for the final result.</p>
<p>To use this stream you will need to have the proper environment keys available depending on the model you are using, like <code>OPENAI_API_KEY</code>, <code>COHERE_API_KEY</code>, <code>HUGGINGFACE_API_KEY</code>, etc,
check it out more details on <a href="https://docs.litellm.ai/docs/completion/supported">LiteLLM docs</a></p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from langstream import Stream, join_final_output
&gt;&gt;&gt; from langstream.contrib import LiteLLMChatStream, LiteLLMChatMessage, LiteLLMChatDelta
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     recipe_stream: Stream[str, str] = LiteLLMChatStream[str, LiteLLMChatDelta](
...         &quot;RecipeStream&quot;,
...         lambda recipe_name: [
...             LiteLLMChatMessage(
...                 role=&quot;system&quot;,
...                 content=&quot;You are Chef Claude, an assistant bot trained on all culinary knowledge of world's most proeminant Michelin Chefs&quot;,
...             ),
...             LiteLLMChatMessage(
...                 role=&quot;user&quot;,
...                 content=f&quot;Hello, could you write me a recipe for {recipe_name}?&quot;,
...             ),
...         ],
...         model=&quot;claude-2&quot;,
...         max_tokens=10,
...     ).map(lambda delta: delta.content)
...
...     return await join_final_output(recipe_stream(&quot;instant noodles&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
&quot;Of course! Here's a simple and delicious recipe&quot;
</code></pre>
<p>You can also pass LiteLLM function schemas in the <code>function</code> argument with all parameter definitions, just like for OpenAI model, but be aware that not all models support it.
Once you pass a <code>function</code> param, the model may then produce a <code>function</code> role <code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatDelta" href="#langstream.contrib.llms.lite_llm.LiteLLMChatDelta">LiteLLMChatDelta</a></code> as output,
using your function, with the <code>content</code> field as a json which you can parse to call an actual function.</p>
<p>Take a look <a href="https://rogeriochaves.github.io/langstream/docs/llms/open_ai_functions">at our OpenAI guide</a> to learn more about LLM function calls in LangStream, it works the same with LiteLLM.</p>
<h2 id="function-call-example">Function Call Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from langstream import Stream, collect_final_output
&gt;&gt;&gt; from langstream.contrib import LiteLLMChatStream, LiteLLMChatMessage, LiteLLMChatDelta
&gt;&gt;&gt; from typing import Literal, Union, Dict
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     def get_current_weather(
...         location: str, format: Literal[&quot;celsius&quot;, &quot;fahrenheit&quot;] = &quot;celsius&quot;
...     ) -&gt; Dict[str, str]:
...         return {
...             &quot;location&quot;: location,
...             &quot;forecast&quot;: &quot;sunny&quot;,
...             &quot;temperature&quot;: &quot;25 C&quot; if format == &quot;celsius&quot; else &quot;77 F&quot;,
...         }
...
...     stream : Stream[str, Union[LiteLLMChatDelta, Dict[str, str]]] = LiteLLMChatStream[str, Union[LiteLLMChatDelta, Dict[str, str]]](
...         &quot;WeatherStream&quot;,
...         lambda user_input: [
...             LiteLLMChatMessage(role=&quot;user&quot;, content=user_input),
...         ],
...         model=&quot;gpt-3.5-turbo&quot;,
...         functions=[
...             {
...                 &quot;name&quot;: &quot;get_current_weather&quot;,
...                 &quot;description&quot;: &quot;Gets the current weather in a given location, use this function for any questions related to the weather&quot;,
...                 &quot;parameters&quot;: {
...                     &quot;type&quot;: &quot;object&quot;,
...                     &quot;properties&quot;: {
...                         &quot;location&quot;: {
...                             &quot;description&quot;: &quot;The city to get the weather, e.g. San Francisco. Guess the location from user messages&quot;,
...                             &quot;type&quot;: &quot;string&quot;,
...                         },
...                         &quot;format&quot;: {
...                             &quot;description&quot;: &quot;A string with the full content of what the given role said&quot;,
...                             &quot;type&quot;: &quot;string&quot;,
...                             &quot;enum&quot;: (&quot;celsius&quot;, &quot;fahrenheit&quot;),
...                         },
...                     },
...                     &quot;required&quot;: [&quot;location&quot;],
...                 },
...             }
...         ],
...         temperature=0,
...     ).map(
...         lambda delta: get_current_weather(**json.loads(delta.content))
...         if delta.role == &quot;function&quot; and delta.name == &quot;get_current_weather&quot;
...         else delta
...     )
...
...     return await collect_final_output(stream(&quot;how is the weather today in Rio de Janeiro?&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
[{'location': 'Rio de Janeiro', 'forecast': 'sunny', 'temperature': '25 C'}]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LiteLLMChatStream(Stream[T, U]):
    &#34;&#34;&#34;
    `LiteLLMChatStream` is a wrapper for [LiteLLM](https://github.com/BerriAI/litellm), which gives you access to OpenAI, Azure OpenAI, Anthropic, Google VertexAI,
    HuggingFace, Replicate, A21, Cohere and a bunch other LLMs all the the same time, all while keeping the standard OpenAI chat interface. Check it out the completion API
    and the available models [on their docs](https://docs.litellm.ai/docs/).

    The `LiteLLMChatStream` takes a lambda function that should return a list of `LiteLLMChatMessage` for the assistant to reply, it is stateless, so it doesn&#39;t keep
    memory of the past chat messages, you will have to handle the memory yourself, you can [follow this guide to get started on memory](https://rogeriochaves.github.io/langstream/docs/llms/memory).

    The `LiteLLMChatStream` also produces `LiteLLMChatDelta` as output, one per token, it contains the `role` that started the output, and then subsequent `content` updates.
    If you want the final content as a string, you will need to use the `.content` property from the delta and accumulate it for the final result.

    To use this stream you will need to have the proper environment keys available depending on the model you are using, like `OPENAI_API_KEY`, `COHERE_API_KEY`, `HUGGINGFACE_API_KEY`, etc,
    check it out more details on [LiteLLM docs](https://docs.litellm.ai/docs/completion/supported)

    Example
    -------

    &gt;&gt;&gt; from langstream import Stream, join_final_output
    &gt;&gt;&gt; from langstream.contrib import LiteLLMChatStream, LiteLLMChatMessage, LiteLLMChatDelta
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_stream: Stream[str, str] = LiteLLMChatStream[str, LiteLLMChatDelta](
    ...         &#34;RecipeStream&#34;,
    ...         lambda recipe_name: [
    ...             LiteLLMChatMessage(
    ...                 role=&#34;system&#34;,
    ...                 content=&#34;You are Chef Claude, an assistant bot trained on all culinary knowledge of world&#39;s most proeminant Michelin Chefs&#34;,
    ...             ),
    ...             LiteLLMChatMessage(
    ...                 role=&#34;user&#34;,
    ...                 content=f&#34;Hello, could you write me a recipe for {recipe_name}?&#34;,
    ...             ),
    ...         ],
    ...         model=&#34;claude-2&#34;,
    ...         max_tokens=10,
    ...     ).map(lambda delta: delta.content)
    ...
    ...     return await join_final_output(recipe_stream(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#34;Of course! Here&#39;s a simple and delicious recipe&#34;

    You can also pass LiteLLM function schemas in the `function` argument with all parameter definitions, just like for OpenAI model, but be aware that not all models support it.
    Once you pass a `function` param, the model may then produce a `function` role `LiteLLMChatDelta` as output,
    using your function, with the `content` field as a json which you can parse to call an actual function.

    Take a look [at our OpenAI guide](https://rogeriochaves.github.io/langstream/docs/llms/open_ai_functions) to learn more about LLM function calls in LangStream, it works the same with LiteLLM.

    Function Call Example
    ---------------------

    &gt;&gt;&gt; from langstream import Stream, collect_final_output
    &gt;&gt;&gt; from langstream.contrib import LiteLLMChatStream, LiteLLMChatMessage, LiteLLMChatDelta
    &gt;&gt;&gt; from typing import Literal, Union, Dict
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     def get_current_weather(
    ...         location: str, format: Literal[&#34;celsius&#34;, &#34;fahrenheit&#34;] = &#34;celsius&#34;
    ...     ) -&gt; Dict[str, str]:
    ...         return {
    ...             &#34;location&#34;: location,
    ...             &#34;forecast&#34;: &#34;sunny&#34;,
    ...             &#34;temperature&#34;: &#34;25 C&#34; if format == &#34;celsius&#34; else &#34;77 F&#34;,
    ...         }
    ...
    ...     stream : Stream[str, Union[LiteLLMChatDelta, Dict[str, str]]] = LiteLLMChatStream[str, Union[LiteLLMChatDelta, Dict[str, str]]](
    ...         &#34;WeatherStream&#34;,
    ...         lambda user_input: [
    ...             LiteLLMChatMessage(role=&#34;user&#34;, content=user_input),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         functions=[
    ...             {
    ...                 &#34;name&#34;: &#34;get_current_weather&#34;,
    ...                 &#34;description&#34;: &#34;Gets the current weather in a given location, use this function for any questions related to the weather&#34;,
    ...                 &#34;parameters&#34;: {
    ...                     &#34;type&#34;: &#34;object&#34;,
    ...                     &#34;properties&#34;: {
    ...                         &#34;location&#34;: {
    ...                             &#34;description&#34;: &#34;The city to get the weather, e.g. San Francisco. Guess the location from user messages&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                         },
    ...                         &#34;format&#34;: {
    ...                             &#34;description&#34;: &#34;A string with the full content of what the given role said&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                             &#34;enum&#34;: (&#34;celsius&#34;, &#34;fahrenheit&#34;),
    ...                         },
    ...                     },
    ...                     &#34;required&#34;: [&#34;location&#34;],
    ...                 },
    ...             }
    ...         ],
    ...         temperature=0,
    ...     ).map(
    ...         lambda delta: get_current_weather(**json.loads(delta.content))
    ...         if delta.role == &#34;function&#34; and delta.name == &#34;get_current_weather&#34;
    ...         else delta
    ...     )
    ...
    ...     return await collect_final_output(stream(&#34;how is the weather today in Rio de Janeiro?&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    [{&#39;location&#39;: &#39;Rio de Janeiro&#39;, &#39;forecast&#39;: &#39;sunny&#39;, &#39;temperature&#39;: &#39;25 C&#39;}]

    &#34;&#34;&#34;

    def __init__(
        self: &#34;LiteLLMChatStream[T, LiteLLMChatDelta]&#34;,
        name: str,
        call: Callable[
            [T],
            List[LiteLLMChatMessage],
        ],
        model: str,
        custom_llm_provider: Optional[str] = None,
        functions: Optional[List[Dict[str, Any]]] = None,
        function_call: Optional[Union[Literal[&#34;none&#34;, &#34;auto&#34;], Dict[str, Any]]] = None,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
        timeout: int = 5,
        retries: int = 3,
    ) -&gt; None:
        async def chat_completion(
            messages: List[LiteLLMChatMessage],
        ) -&gt; AsyncGenerator[StreamOutput[LiteLLMChatDelta], None]:
            loop = asyncio.get_event_loop()

            @retry(tries=retries)
            def get_completions():
                function_kwargs = {}
                if functions is not None:
                    function_kwargs[&#34;functions&#34;] = functions
                if function_call is not None:
                    function_kwargs[&#34;function_call&#34;] = function_call

                litellm = importlib.import_module(&#34;litellm&#34;)
                # import litellm

                return litellm.completion(
                    request_timeout=timeout,
                    model=model,
                    custom_llm_provider=custom_llm_provider,
                    messages=[m.to_dict() for m in messages],
                    temperature=temperature,  # type: ignore (why is their type int?)
                    stream=True,
                    max_tokens=max_tokens,  # type: ignore (why is their type float?)
                    **function_kwargs,
                )

            completions = await loop.run_in_executor(None, get_completions)

            pending_function_call: Optional[LiteLLMChatDelta] = None

            completions = (
                completions if hasattr(completions, &#34;__iter__&#34;) else [completions]
            )
            # from litellm import ModelResponse
            # from litellm.utils import StreamingChoices

            for output in completions:
                # output = cast(ModelResponse, output)
                if len(output.choices) == 0:
                    continue

                # choices = cast(List[StreamingChoices], output.choices)
                choices = output.choices
                delta = choices[0].delta
                if not delta:
                    continue

                delta_function_call = delta.model_dump().get(&#34;function_call&#34;)
                if delta_function_call is not None:
                    role = delta.role
                    function_name: Optional[str] = (
                        delta_function_call[&#34;name&#34;]
                        if &#34;name&#34; in delta_function_call
                        else None
                    )
                    function_arguments: Optional[str] = (
                        delta_function_call[&#34;arguments&#34;]
                        if &#34;arguments&#34; in delta_function_call
                        else None
                    )

                    if function_name is not None:
                        pending_function_call = LiteLLMChatDelta(
                            role=&#34;function&#34;,
                            name=function_name,
                            content=function_arguments or &#34;&#34;,
                        )
                    elif (
                        pending_function_call is not None
                        and function_arguments is not None
                    ):
                        pending_function_call.content += function_arguments
                elif delta.content is not None:
                    role = cast(
                        Union[Literal[&#34;assistant&#34;, &#34;function&#34;], None], delta.role
                    )
                    yield self._output_wrap(
                        LiteLLMChatDelta(
                            role=role,
                            content=delta.content,
                        )
                    )
                else:
                    if pending_function_call:
                        yield self._output_wrap(pending_function_call)
                        pending_function_call = None
            if pending_function_call:
                yield self._output_wrap(pending_function_call)
                pending_function_call = None

        super().__init__(
            name,
            lambda input: cast(AsyncGenerator[U, None], chat_completion(call(input))),
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="langstream.core.stream.Stream" href="../../core/stream.html#langstream.core.stream.Stream">Stream</a></li>
<li>typing.Generic</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="langstream.core.stream.Stream" href="../../core/stream.html#langstream.core.stream.Stream">Stream</a></b></code>:
<ul class="hlist">
<li><code><a title="langstream.core.stream.Stream.and_then" href="../../core/stream.html#langstream.core.stream.Stream.and_then">and_then</a></code></li>
<li><code><a title="langstream.core.stream.Stream.collect" href="../../core/stream.html#langstream.core.stream.Stream.collect">collect</a></code></li>
<li><code><a title="langstream.core.stream.Stream.filter" href="../../core/stream.html#langstream.core.stream.Stream.filter">filter</a></code></li>
<li><code><a title="langstream.core.stream.Stream.gather" href="../../core/stream.html#langstream.core.stream.Stream.gather">gather</a></code></li>
<li><code><a title="langstream.core.stream.Stream.join" href="../../core/stream.html#langstream.core.stream.Stream.join">join</a></code></li>
<li><code><a title="langstream.core.stream.Stream.map" href="../../core/stream.html#langstream.core.stream.Stream.map">map</a></code></li>
<li><code><a title="langstream.core.stream.Stream.on_error" href="../../core/stream.html#langstream.core.stream.Stream.on_error">on_error</a></code></li>
<li><code><a title="langstream.core.stream.Stream.pipe" href="../../core/stream.html#langstream.core.stream.Stream.pipe">pipe</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/langstream/docs/intro" style="color: #000">‚Üê Back to Docs</a>
<h1>ü™Ωüîó LangStream API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="langstream.contrib.llms" href="index.html">langstream.contrib.llms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatDelta" href="#langstream.contrib.llms.lite_llm.LiteLLMChatDelta">LiteLLMChatDelta</a></code></h4>
<ul class="">
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatDelta.content" href="#langstream.contrib.llms.lite_llm.LiteLLMChatDelta.content">content</a></code></li>
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatDelta.name" href="#langstream.contrib.llms.lite_llm.LiteLLMChatDelta.name">name</a></code></li>
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatDelta.role" href="#langstream.contrib.llms.lite_llm.LiteLLMChatDelta.role">role</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage">LiteLLMChatMessage</a></code></h4>
<ul class="">
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.content" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage.content">content</a></code></li>
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.name" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage.name">name</a></code></li>
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.role" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage.role">role</a></code></li>
<li><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatMessage.to_dict" href="#langstream.contrib.llms.lite_llm.LiteLLMChatMessage.to_dict">to_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langstream.contrib.llms.lite_llm.LiteLLMChatStream" href="#langstream.contrib.llms.lite_llm.LiteLLMChatStream">LiteLLMChatStream</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>