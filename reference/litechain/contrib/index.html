<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>litechain.contrib API documentation</title>
<meta name="description" content="The Contrib module is where all the other chains and integrations that build on top of
core Chain module live. Here you can import the LLMs you want …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🪽</text></svg>" data-rh="true">
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/litechain/"
><b class="navbar__title text--truncate">🪽🔗 LiteChain</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/litechain/docs/intro"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/litechain/reference/litechain/index.html"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/rogeriochaves/litechain"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>litechain.contrib</code></h1>
</header>
<section id="section-intro">
<p>The Contrib module is where all the other chains and integrations that build on top of
core Chain module live. Here you can import the LLMs you want to use.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; from litechain.contrib import OpenAIChatChain, GPT4AllChain # etc
</code></pre>
<hr>
<p>Here you can find the reference and code examples, for further tutorials and use cases, consult the <a href="https://github.com/rogeriochaves/litechain">documentation</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The Contrib module is where all the other chains and integrations that build on top of
core Chain module live. Here you can import the LLMs you want to use.

&gt;&gt;&gt; from litechain.contrib import OpenAIChatChain, GPT4AllChain # etc

---

Here you can find the reference and code examples, for further tutorials and use cases, consult the [documentation](https://github.com/rogeriochaves/litechain).
&#34;&#34;&#34;

from litechain.contrib.llms.open_ai import (
    OpenAICompletionChain,
    OpenAIChatChain,
    OpenAIChatMessage,
    OpenAIChatDelta,
)
from litechain.contrib.llms.gpt4all_chain import GPT4AllChain

__all__ = (
    &#34;OpenAICompletionChain&#34;,
    &#34;OpenAIChatChain&#34;,
    &#34;OpenAIChatMessage&#34;,
    &#34;OpenAIChatDelta&#34;,
    &#34;GPT4AllChain&#34;,
)</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="litechain.contrib.llms" href="llms/index.html">litechain.contrib.llms</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="litechain.contrib.GPT4AllChain"><code class="flex name class">
<span>class <span class="ident">GPT4AllChain</span></span>
<span>(</span><span>name: str, call: Callable[[~T], str], model: str, temperature: float = 0, max_tokens: int = 200, top_k=40, top_p=0.1, repeat_penalty=1.18, repeat_last_n=64, n_batch=8, n_threads: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>GPT4AllChain is a Chain that allows you to run local LLMs easily
using <a href="https://gpt4all.io/">GPT4All</a> model.</p>
<p><a href="https://gpt4all.io/">GPT4All</a> is a project that focuses on making
the LLM models very small and very fast to be able to run in any computer
without GPUs. Check out more about the project <a href="https://gpt4all.io/">here</a>.</p>
<p>You can use it as any other chain, on the first use, it will download the model
(a few GB). Alternatively, you can point to a locally downloaded model.bin file.</p>
<p>There are serveral parameters you can use to adjust the model output such as
<code>temperature</code>, <code>max_tokens</code>, <code>top_k</code>, <code>repeat_penalty</code>, etc, you can read more
about them <a href="https://docs.gpt4all.io/gpt4all_python.html#generation-parameters">here</a>.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from litechain import join_final_output
&gt;&gt;&gt; from litechain.contrib import GPT4AllChain
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     greet_chain = GPT4AllChain[str, str](
...         &quot;GreetingChain&quot;,
...         lambda name: f&quot;### User: Hello, my name is {name}. How is it going?\n\n### Response:&quot;,
...         model=&quot;orca-mini-3b.ggmlv3.q4_0.bin&quot;,
...         temperature=0,
...     )
...
...     return await join_final_output(greet_chain(&quot;Alice&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+ELLIPSIS +SKIP
Found model file at ...
&quot; I'm doing well, thank you for asking! How about you?&quot;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GPT4AllChain(Chain[T, U]):
    &#34;&#34;&#34;
    GPT4AllChain is a Chain that allows you to run local LLMs easily
    using [GPT4All](https://gpt4all.io/) model.

    [GPT4All](https://gpt4all.io/) is a project that focuses on making
    the LLM models very small and very fast to be able to run in any computer
    without GPUs. Check out more about the project [here](https://gpt4all.io/).

    You can use it as any other chain, on the first use, it will download the model
    (a few GB). Alternatively, you can point to a locally downloaded model.bin file.

    There are serveral parameters you can use to adjust the model output such as
    `temperature`, `max_tokens`, `top_k`, `repeat_penalty`, etc, you can read more
    about them [here](https://docs.gpt4all.io/gpt4all_python.html#generation-parameters).

    Example
    -------

    &gt;&gt;&gt; from litechain import join_final_output
    &gt;&gt;&gt; from litechain.contrib import GPT4AllChain
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     greet_chain = GPT4AllChain[str, str](
    ...         &#34;GreetingChain&#34;,
    ...         lambda name: f&#34;### User: Hello, my name is {name}. How is it going?\\n\\n### Response:&#34;,
    ...         model=&#34;orca-mini-3b.ggmlv3.q4_0.bin&#34;,
    ...         temperature=0,
    ...     )
    ...
    ...     return await join_final_output(greet_chain(&#34;Alice&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+ELLIPSIS +SKIP
    Found model file at ...
    &#34; I&#39;m doing well, thank you for asking! How about you?&#34;

    &#34;&#34;&#34;

    def __init__(
        self: &#34;GPT4AllChain[T, str]&#34;,
        name: str,
        call: Callable[
            [T],
            str,
        ],
        model: str,
        temperature: float = 0,
        max_tokens: int = 200,
        top_k=40,
        top_p=0.1,
        repeat_penalty=1.18,
        repeat_last_n=64,
        n_batch=8,
        n_threads: Optional[int] = None,
    ) -&gt; None:
        self.name = name
        gpt4all = GPT4All(model, n_threads=n_threads)

        async def generate(prompt: str) -&gt; AsyncGenerator[str, None]:
            loop = asyncio.get_event_loop()

            def get_outputs() -&gt; Iterable[str]:
                return gpt4all.generate(
                    prompt,
                    streaming=True,
                    temp=temperature,
                    max_tokens=max_tokens,
                    top_k=top_k,
                    top_p=top_p,
                    repeat_penalty=repeat_penalty,
                    repeat_last_n=repeat_last_n,
                    n_batch=n_batch,
                )

            outputs = await loop.run_in_executor(None, get_outputs)

            for output in outputs:
                yield output

        self._call = lambda input: generate(call(input))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="litechain.core.chain.Chain" href="../core/chain.html#litechain.core.chain.Chain">Chain</a></li>
<li>typing.Generic</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="litechain.core.chain.Chain" href="../core/chain.html#litechain.core.chain.Chain">Chain</a></b></code>:
<ul class="hlist">
<li><code><a title="litechain.core.chain.Chain.and_then" href="../core/chain.html#litechain.core.chain.Chain.and_then">and_then</a></code></li>
<li><code><a title="litechain.core.chain.Chain.collect" href="../core/chain.html#litechain.core.chain.Chain.collect">collect</a></code></li>
<li><code><a title="litechain.core.chain.Chain.gather" href="../core/chain.html#litechain.core.chain.Chain.gather">gather</a></code></li>
<li><code><a title="litechain.core.chain.Chain.join" href="../core/chain.html#litechain.core.chain.Chain.join">join</a></code></li>
<li><code><a title="litechain.core.chain.Chain.map" href="../core/chain.html#litechain.core.chain.Chain.map">map</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="litechain.contrib.OpenAIChatChain"><code class="flex name class">
<span>class <span class="ident">OpenAIChatChain</span></span>
<span>(</span><span>name: str, call: Callable[[~T], List[<a title="litechain.contrib.llms.open_ai.OpenAIChatMessage" href="llms/open_ai.html#litechain.contrib.llms.open_ai.OpenAIChatMessage">OpenAIChatMessage</a>]], model: str, functions: Optional[List[Dict[str, Any]]] = None, function_call: Union[Literal['none', 'auto'], str, ForwardRef(None)] = None, temperature: Optional[float] = 0, max_tokens: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code> gives you access to the more powerful LLMs from OpenAI, like <code>gpt-3.5-turbo</code> and <code>gpt-4</code>, they are structured in a chat format with roles.</p>
<p>The <code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code> takes a lambda function that should return a list of <code><a title="litechain.contrib.OpenAIChatMessage" href="#litechain.contrib.OpenAIChatMessage">OpenAIChatMessage</a></code> for the assistant to reply, it is stateless, so it doesn't keep
memory of the past chat messages, you will have to handle the memory yourself, you can <a href="https://rogeriochaves.github.io/litechain/docs/llms/memory">follow this guide to get started on memory</a>.</p>
<p>The <code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code> also produces <code><a title="litechain.contrib.OpenAIChatDelta" href="#litechain.contrib.OpenAIChatDelta">OpenAIChatDelta</a></code> as output, one per token, it contains the <code>role</code> that started the output, and then subsequent <code>content</code> updates.
If you want the final content as a string, you will need to use the <code>.content</code> property from the delta and accumulate it for the final result.</p>
<p>To use this chain you will need an <code>OPENAI_API_KEY</code> environment variable to be available, and then you can generate chat completions out of it.</p>
<p>You can read more about the chat completion API on <a href="https://platform.openai.com/docs/api-reference/chat">OpenAI API reference</a></p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from litechain import Chain, join_final_output
&gt;&gt;&gt; from litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     recipe_chain: Chain[str, str] = OpenAIChatChain[str, OpenAIChatDelta](
...         &quot;RecipeChain&quot;,
...         lambda recipe_name: [
...             OpenAIChatMessage(
...                 role=&quot;system&quot;,
...                 content=&quot;You are ChefGPT, an assistant bot trained on all culinary knowledge of world's most proeminant Michelin Chefs&quot;,
...             ),
...             OpenAIChatMessage(
...                 role=&quot;user&quot;,
...                 content=f&quot;Hello, could you write me a recipe for {recipe_name}?&quot;,
...             ),
...         ],
...         model=&quot;gpt-3.5-turbo&quot;,
...         max_tokens=10,
...     ).map(lambda delta: delta.content)
...
...     return await join_final_output(recipe_chain(&quot;instant noodles&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
&quot;Of course! Here's a simple and delicious recipe&quot;
</code></pre>
<p>You can also pass OpenAI function schemas in the <code>function</code> argument with all parameter definitions, the model may then produce a <code>function</code> role <code><a title="litechain.contrib.OpenAIChatDelta" href="#litechain.contrib.OpenAIChatDelta">OpenAIChatDelta</a></code>,
using your function, with the <code>content</code> field as a json which you can parse to call an actual function.</p>
<p>Take a look <a href="https://rogeriochaves.github.io/litechain/docs/llms/open_ai_functions">at our guide</a> to learn more about OpenAI function calls in LiteChain.</p>
<h2 id="function-call-example">Function Call Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from litechain import Chain, collect_final_output
&gt;&gt;&gt; from litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta
&gt;&gt;&gt; from typing import Literal, Union, Dict
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     def get_current_weather(
...         location: str, format: Literal[&quot;celsius&quot;, &quot;fahrenheit&quot;] = &quot;celsius&quot;
...     ) -&gt; Dict[str, str]:
...         return {
...             &quot;location&quot;: location,
...             &quot;forecast&quot;: &quot;sunny&quot;,
...             &quot;temperature&quot;: &quot;25 C&quot; if format == &quot;celsius&quot; else &quot;77 F&quot;,
...         }
...
...     chain : Chain[str, Union[OpenAIChatDelta, Dict[str, str]]] = OpenAIChatChain[str, Union[OpenAIChatDelta, Dict[str, str]]](
...         &quot;WeatherChain&quot;,
...         lambda user_input: [
...             OpenAIChatMessage(role=&quot;user&quot;, content=user_input),
...         ],
...         model=&quot;gpt-3.5-turbo&quot;,
...         functions=[
...             {
...                 &quot;name&quot;: &quot;get_current_weather&quot;,
...                 &quot;description&quot;: &quot;Gets the current weather in a given location, use this function for any questions related to the weather&quot;,
...                 &quot;parameters&quot;: {
...                     &quot;type&quot;: &quot;object&quot;,
...                     &quot;properties&quot;: {
...                         &quot;location&quot;: {
...                             &quot;description&quot;: &quot;The city to get the weather, e.g. San Francisco. Guess the location from user messages&quot;,
...                             &quot;type&quot;: &quot;string&quot;,
...                         },
...                         &quot;format&quot;: {
...                             &quot;description&quot;: &quot;A string with the full content of what the given role said&quot;,
...                             &quot;type&quot;: &quot;string&quot;,
...                             &quot;enum&quot;: (&quot;celsius&quot;, &quot;fahrenheit&quot;),
...                         },
...                     },
...                 },
...                 &quot;required&quot;: [&quot;location&quot;],
...             }
...         ],
...         temperature=0,
...     ).map(
...         lambda delta: get_current_weather(**json.loads(delta.content))
...         if delta.role == &quot;function&quot; and delta.name == &quot;get_current_weather&quot;
...         else delta
...     )
...
...     return await collect_final_output(chain(&quot;how is the weather today in Rio de Janeiro?&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
[{'location': 'Rio de Janeiro', 'forecast': 'sunny', 'temperature': '25 C'}]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OpenAIChatChain(Chain[T, U]):
    &#34;&#34;&#34;
    `OpenAIChatChain` gives you access to the more powerful LLMs from OpenAI, like `gpt-3.5-turbo` and `gpt-4`, they are structured in a chat format with roles.

    The `OpenAIChatChain` takes a lambda function that should return a list of `OpenAIChatMessage` for the assistant to reply, it is stateless, so it doesn&#39;t keep
    memory of the past chat messages, you will have to handle the memory yourself, you can [follow this guide to get started on memory](https://rogeriochaves.github.io/litechain/docs/llms/memory).

    The `OpenAIChatChain` also produces `OpenAIChatDelta` as output, one per token, it contains the `role` that started the output, and then subsequent `content` updates.
    If you want the final content as a string, you will need to use the `.content` property from the delta and accumulate it for the final result.

    To use this chain you will need an `OPENAI_API_KEY` environment variable to be available, and then you can generate chat completions out of it.

    You can read more about the chat completion API on [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat)

    Example
    -------

    &gt;&gt;&gt; from litechain import Chain, join_final_output
    &gt;&gt;&gt; from litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_chain: Chain[str, str] = OpenAIChatChain[str, OpenAIChatDelta](
    ...         &#34;RecipeChain&#34;,
    ...         lambda recipe_name: [
    ...             OpenAIChatMessage(
    ...                 role=&#34;system&#34;,
    ...                 content=&#34;You are ChefGPT, an assistant bot trained on all culinary knowledge of world&#39;s most proeminant Michelin Chefs&#34;,
    ...             ),
    ...             OpenAIChatMessage(
    ...                 role=&#34;user&#34;,
    ...                 content=f&#34;Hello, could you write me a recipe for {recipe_name}?&#34;,
    ...             ),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         max_tokens=10,
    ...     ).map(lambda delta: delta.content)
    ...
    ...     return await join_final_output(recipe_chain(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#34;Of course! Here&#39;s a simple and delicious recipe&#34;

    You can also pass OpenAI function schemas in the `function` argument with all parameter definitions, the model may then produce a `function` role `OpenAIChatDelta`,
    using your function, with the `content` field as a json which you can parse to call an actual function.

    Take a look [at our guide](https://rogeriochaves.github.io/litechain/docs/llms/open_ai_functions) to learn more about OpenAI function calls in LiteChain.

    Function Call Example
    ---------------------

    &gt;&gt;&gt; from litechain import Chain, collect_final_output
    &gt;&gt;&gt; from litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta
    &gt;&gt;&gt; from typing import Literal, Union, Dict
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     def get_current_weather(
    ...         location: str, format: Literal[&#34;celsius&#34;, &#34;fahrenheit&#34;] = &#34;celsius&#34;
    ...     ) -&gt; Dict[str, str]:
    ...         return {
    ...             &#34;location&#34;: location,
    ...             &#34;forecast&#34;: &#34;sunny&#34;,
    ...             &#34;temperature&#34;: &#34;25 C&#34; if format == &#34;celsius&#34; else &#34;77 F&#34;,
    ...         }
    ...
    ...     chain : Chain[str, Union[OpenAIChatDelta, Dict[str, str]]] = OpenAIChatChain[str, Union[OpenAIChatDelta, Dict[str, str]]](
    ...         &#34;WeatherChain&#34;,
    ...         lambda user_input: [
    ...             OpenAIChatMessage(role=&#34;user&#34;, content=user_input),
    ...         ],
    ...         model=&#34;gpt-3.5-turbo&#34;,
    ...         functions=[
    ...             {
    ...                 &#34;name&#34;: &#34;get_current_weather&#34;,
    ...                 &#34;description&#34;: &#34;Gets the current weather in a given location, use this function for any questions related to the weather&#34;,
    ...                 &#34;parameters&#34;: {
    ...                     &#34;type&#34;: &#34;object&#34;,
    ...                     &#34;properties&#34;: {
    ...                         &#34;location&#34;: {
    ...                             &#34;description&#34;: &#34;The city to get the weather, e.g. San Francisco. Guess the location from user messages&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                         },
    ...                         &#34;format&#34;: {
    ...                             &#34;description&#34;: &#34;A string with the full content of what the given role said&#34;,
    ...                             &#34;type&#34;: &#34;string&#34;,
    ...                             &#34;enum&#34;: (&#34;celsius&#34;, &#34;fahrenheit&#34;),
    ...                         },
    ...                     },
    ...                 },
    ...                 &#34;required&#34;: [&#34;location&#34;],
    ...             }
    ...         ],
    ...         temperature=0,
    ...     ).map(
    ...         lambda delta: get_current_weather(**json.loads(delta.content))
    ...         if delta.role == &#34;function&#34; and delta.name == &#34;get_current_weather&#34;
    ...         else delta
    ...     )
    ...
    ...     return await collect_final_output(chain(&#34;how is the weather today in Rio de Janeiro?&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    [{&#39;location&#39;: &#39;Rio de Janeiro&#39;, &#39;forecast&#39;: &#39;sunny&#39;, &#39;temperature&#39;: &#39;25 C&#39;}]

    &#34;&#34;&#34;

    def __init__(
        self: &#34;OpenAIChatChain[T, Union[OpenAIChatDelta, V]]&#34;,
        name: str,
        call: Callable[
            [T],
            List[OpenAIChatMessage],
        ],
        model: str,
        functions: Optional[List[Dict[str, Any]]] = None,
        function_call: Optional[Union[Literal[&#34;none&#34;, &#34;auto&#34;], str]] = None,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
    ) -&gt; None:
        self.name = name

        async def chat_completion(
            messages: List[OpenAIChatMessage],
        ) -&gt; AsyncGenerator[ChainOutput[Union[OpenAIChatDelta, V], Any], Any]:
            loop = asyncio.get_event_loop()

            def get_completions():
                function_kwargs = {}
                if functions is not None:
                    function_kwargs[&#34;functions&#34;] = functions
                if function_call is not None:
                    function_kwargs[&#34;function_call&#34;] = function_call

                return openai.ChatCompletion.create(
                    model=model,
                    messages=[m.to_dict() for m in messages],
                    temperature=temperature,
                    stream=True,
                    max_tokens=max_tokens,
                    **function_kwargs,
                )

            completions = await loop.run_in_executor(None, get_completions)

            pending_function_call: Optional[OpenAIChatDelta] = None

            for output in completions:
                output = cast(dict, output)
                if &#34;choices&#34; not in output:
                    continue

                if len(output[&#34;choices&#34;]) == 0:
                    continue

                if &#34;delta&#34; not in output[&#34;choices&#34;][0]:
                    continue

                if &#34;function_call&#34; in output[&#34;choices&#34;][0][&#34;delta&#34;]:
                    delta = output[&#34;choices&#34;][0][&#34;delta&#34;]
                    role = delta[&#34;role&#34;] if &#34;role&#34; in delta else None
                    function_name: Optional[str] = delta[&#34;function_call&#34;].get(&#34;name&#34;)
                    function_arguments: Optional[str] = delta[&#34;function_call&#34;].get(
                        &#34;arguments&#34;
                    )

                    if function_name is not None:
                        pending_function_call = OpenAIChatDelta(
                            role=&#34;function&#34;,
                            name=function_name,
                            content=function_arguments or &#34;&#34;,
                        )
                    elif (
                        pending_function_call is not None
                        and function_arguments is not None
                    ):
                        pending_function_call.content += function_arguments
                elif &#34;content&#34; in output[&#34;choices&#34;][0][&#34;delta&#34;]:
                    delta = output[&#34;choices&#34;][0][&#34;delta&#34;]
                    role = delta[&#34;role&#34;] if &#34;role&#34; in delta else None
                    yield self._output_wrap(
                        OpenAIChatDelta(
                            role=role,
                            content=delta[&#34;content&#34;],
                        )
                    )
                else:
                    if pending_function_call:
                        yield self._output_wrap(pending_function_call)
                        pending_function_call = None
            if pending_function_call:
                yield self._output_wrap(pending_function_call)
                pending_function_call = None

        self._call = lambda input: chat_completion(call(input))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="litechain.core.chain.Chain" href="../core/chain.html#litechain.core.chain.Chain">Chain</a></li>
<li>typing.Generic</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="litechain.core.chain.Chain" href="../core/chain.html#litechain.core.chain.Chain">Chain</a></b></code>:
<ul class="hlist">
<li><code><a title="litechain.core.chain.Chain.and_then" href="../core/chain.html#litechain.core.chain.Chain.and_then">and_then</a></code></li>
<li><code><a title="litechain.core.chain.Chain.collect" href="../core/chain.html#litechain.core.chain.Chain.collect">collect</a></code></li>
<li><code><a title="litechain.core.chain.Chain.gather" href="../core/chain.html#litechain.core.chain.Chain.gather">gather</a></code></li>
<li><code><a title="litechain.core.chain.Chain.join" href="../core/chain.html#litechain.core.chain.Chain.join">join</a></code></li>
<li><code><a title="litechain.core.chain.Chain.map" href="../core/chain.html#litechain.core.chain.Chain.map">map</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="litechain.contrib.OpenAIChatDelta"><code class="flex name class">
<span>class <span class="ident">OpenAIChatDelta</span></span>
<span>(</span><span>role: Optional[Literal['assistant', 'function']], content: str, name: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>OpenAIChatDelta is a data class that represents the output of an <code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code>.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>Optional[Literal["assistant", "function"]]</code></dt>
<dd>The role of the output message, the first message will have the role, while
the subsequent partial content output ones will have the role as <code>None</code>.
For now the only possible values it will have is either None or <code>"assistant"</code></dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>The name is used for when <code>role</code> is <code>"function"</code>, it represents the name of the function that was called</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the partial content being outputted by the LLM, this generally
translate to each token the LLM is producing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class OpenAIChatDelta:
    &#34;&#34;&#34;
    OpenAIChatDelta is a data class that represents the output of an `OpenAIChatChain`.

    Attributes
    ----------
    role : Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
        The role of the output message, the first message will have the role, while
        the subsequent partial content output ones will have the role as `None`.
        For now the only possible values it will have is either None or `&#34;assistant&#34;`

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the partial content being outputted by the LLM, this generally
        translate to each token the LLM is producing

    &#34;&#34;&#34;

    role: Optional[Literal[&#34;assistant&#34;, &#34;function&#34;]]
    content: str
    name: Optional[str] = None

    def __chain_debug__(self):
        name = &#34;&#34;
        if self.name:
            name = f&#34; {self.name}&#34;
        if self.role is not None:
            print(f&#34;{Fore.YELLOW}{self.role.capitalize()}{name}:{Fore.RESET} &#34;, end=&#34;&#34;)
        print(
            self.content,
            end=&#34;&#34;,
            flush=True,
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="litechain.contrib.OpenAIChatDelta.content"><code class="name">var <span class="ident">content</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="litechain.contrib.OpenAIChatDelta.name"><code class="name">var <span class="ident">name</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="litechain.contrib.OpenAIChatDelta.role"><code class="name">var <span class="ident">role</span> : Optional[Literal['assistant', 'function']]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="litechain.contrib.OpenAIChatMessage"><code class="flex name class">
<span>class <span class="ident">OpenAIChatMessage</span></span>
<span>(</span><span>role: Literal['system', 'user', 'assistant', 'function'], content: str, name: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>OpenAIChatMessage is a data class that represents a chat message for building <code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code> prompt.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>Literal["system", "user", "assistant", "function"]</code></dt>
<dd>The role of who sent this message in the chat, can be one of <code>"system"</code>, <code>"user"</code>, <code>"assistant"</code> or "function"</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>The name is used for when <code>role</code> is <code>"function"</code>, it represents the name of the function that was called</dd>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the full content of what the given role said</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class OpenAIChatMessage:
    &#34;&#34;&#34;
    OpenAIChatMessage is a data class that represents a chat message for building `OpenAIChatChain` prompt.

    Attributes
    ----------
    role : Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
        The role of who sent this message in the chat, can be one of `&#34;system&#34;`, `&#34;user&#34;`, `&#34;assistant&#34;` or &#34;function&#34;

    name: Optional[str]
        The name is used for when `role` is `&#34;function&#34;`, it represents the name of the function that was called

    content : str
        A string with the full content of what the given role said

    &#34;&#34;&#34;

    role: Literal[&#34;system&#34;, &#34;user&#34;, &#34;assistant&#34;, &#34;function&#34;]
    content: str
    name: Optional[str] = None

    def to_dict(self):
        return {k: v for k, v in self.__dict__.items() if v is not None}</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="litechain.contrib.OpenAIChatMessage.content"><code class="name">var <span class="ident">content</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="litechain.contrib.OpenAIChatMessage.name"><code class="name">var <span class="ident">name</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="litechain.contrib.OpenAIChatMessage.role"><code class="name">var <span class="ident">role</span> : Literal['system', 'user', 'assistant', 'function']</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="litechain.contrib.OpenAIChatMessage.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self):
    return {k: v for k, v in self.__dict__.items() if v is not None}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="litechain.contrib.OpenAICompletionChain"><code class="flex name class">
<span>class <span class="ident">OpenAICompletionChain</span></span>
<span>(</span><span>name: str, call: Callable[[~T], str], model: str, temperature: Optional[float] = 0, max_tokens: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="litechain.contrib.OpenAICompletionChain" href="#litechain.contrib.OpenAICompletionChain">OpenAICompletionChain</a></code> uses the most simple LLMs from OpenAI based on GPT-3 for text completion, if you are looking for ChatCompletion, take a look at <code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code>.</p>
<p>The <code><a title="litechain.contrib.OpenAICompletionChain" href="#litechain.contrib.OpenAICompletionChain">OpenAICompletionChain</a></code> takes a lambda function that should return a string with the prompt for completion.</p>
<p>To use this chain you will need an <code>OPENAI_API_KEY</code> environment variable to be available, and then you can generate completions out of it.</p>
<p>You can read more about the completion API on <a href="https://platform.openai.com/docs/api-reference/completions">OpenAI API reference</a></p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from litechain import join_final_output
&gt;&gt;&gt; from litechain.contrib import OpenAICompletionChain
&gt;&gt;&gt; import asyncio
...
&gt;&gt;&gt; async def example():
...     recipe_chain = OpenAICompletionChain[str, str](
...         &quot;RecipeChain&quot;,
...         lambda recipe_name: f&quot;Here is my {recipe_name} recipe: &quot;,
...         model=&quot;ada&quot;,
...     )
...
...     return await join_final_output(recipe_chain(&quot;instant noodles&quot;))
...
&gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
'【Instant Noodles】\n\nIngredients:\n\n1 cup of water'
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OpenAICompletionChain(Chain[T, U]):
    &#34;&#34;&#34;
    `OpenAICompletionChain` uses the most simple LLMs from OpenAI based on GPT-3 for text completion, if you are looking for ChatCompletion, take a look at `OpenAIChatChain`.

    The `OpenAICompletionChain` takes a lambda function that should return a string with the prompt for completion.

    To use this chain you will need an `OPENAI_API_KEY` environment variable to be available, and then you can generate completions out of it.

    You can read more about the completion API on [OpenAI API reference](https://platform.openai.com/docs/api-reference/completions)

    Example
    -------

    &gt;&gt;&gt; from litechain import join_final_output
    &gt;&gt;&gt; from litechain.contrib import OpenAICompletionChain
    &gt;&gt;&gt; import asyncio
    ...
    &gt;&gt;&gt; async def example():
    ...     recipe_chain = OpenAICompletionChain[str, str](
    ...         &#34;RecipeChain&#34;,
    ...         lambda recipe_name: f&#34;Here is my {recipe_name} recipe: &#34;,
    ...         model=&#34;ada&#34;,
    ...     )
    ...
    ...     return await join_final_output(recipe_chain(&#34;instant noodles&#34;))
    ...
    &gt;&gt;&gt; asyncio.run(example()) # doctest:+SKIP
    &#39;【Instant Noodles】\\n\\nIngredients:\\n\\n1 cup of water&#39;

    &#34;&#34;&#34;

    def __init__(
        self: &#34;OpenAICompletionChain[T, str]&#34;,
        name: str,
        call: Callable[
            [T],
            str,
        ],
        model: str,
        temperature: Optional[float] = 0,
        max_tokens: Optional[int] = None,
    ) -&gt; None:
        self.name = name

        async def completion(prompt: str) -&gt; AsyncGenerator[str, None]:
            loop = asyncio.get_event_loop()

            def get_completions():
                return openai.Completion.create(
                    model=model,
                    prompt=prompt,
                    temperature=temperature,
                    stream=True,
                    max_tokens=max_tokens,
                )

            completions = await loop.run_in_executor(None, get_completions)

            for output in completions:
                output = cast(dict, output)
                if &#34;choices&#34; in output:
                    if len(output[&#34;choices&#34;]) &gt; 0:
                        if &#34;text&#34; in output[&#34;choices&#34;][0]:
                            yield output[&#34;choices&#34;][0][&#34;text&#34;]

        self._call = lambda input: completion(call(input))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="litechain.core.chain.Chain" href="../core/chain.html#litechain.core.chain.Chain">Chain</a></li>
<li>typing.Generic</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="litechain.core.chain.Chain" href="../core/chain.html#litechain.core.chain.Chain">Chain</a></b></code>:
<ul class="hlist">
<li><code><a title="litechain.core.chain.Chain.and_then" href="../core/chain.html#litechain.core.chain.Chain.and_then">and_then</a></code></li>
<li><code><a title="litechain.core.chain.Chain.collect" href="../core/chain.html#litechain.core.chain.Chain.collect">collect</a></code></li>
<li><code><a title="litechain.core.chain.Chain.gather" href="../core/chain.html#litechain.core.chain.Chain.gather">gather</a></code></li>
<li><code><a title="litechain.core.chain.Chain.join" href="../core/chain.html#litechain.core.chain.Chain.join">join</a></code></li>
<li><code><a title="litechain.core.chain.Chain.map" href="../core/chain.html#litechain.core.chain.Chain.map">map</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/litechain/docs/intro" style="color: #000">← Back to Docs</a>
<h1>🪽🔗 LiteChain API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="litechain" href="../index.html">litechain</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="litechain.contrib.llms" href="llms/index.html">litechain.contrib.llms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="litechain.contrib.GPT4AllChain" href="#litechain.contrib.GPT4AllChain">GPT4AllChain</a></code></h4>
</li>
<li>
<h4><code><a title="litechain.contrib.OpenAIChatChain" href="#litechain.contrib.OpenAIChatChain">OpenAIChatChain</a></code></h4>
</li>
<li>
<h4><code><a title="litechain.contrib.OpenAIChatDelta" href="#litechain.contrib.OpenAIChatDelta">OpenAIChatDelta</a></code></h4>
<ul class="">
<li><code><a title="litechain.contrib.OpenAIChatDelta.content" href="#litechain.contrib.OpenAIChatDelta.content">content</a></code></li>
<li><code><a title="litechain.contrib.OpenAIChatDelta.name" href="#litechain.contrib.OpenAIChatDelta.name">name</a></code></li>
<li><code><a title="litechain.contrib.OpenAIChatDelta.role" href="#litechain.contrib.OpenAIChatDelta.role">role</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="litechain.contrib.OpenAIChatMessage" href="#litechain.contrib.OpenAIChatMessage">OpenAIChatMessage</a></code></h4>
<ul class="">
<li><code><a title="litechain.contrib.OpenAIChatMessage.content" href="#litechain.contrib.OpenAIChatMessage.content">content</a></code></li>
<li><code><a title="litechain.contrib.OpenAIChatMessage.name" href="#litechain.contrib.OpenAIChatMessage.name">name</a></code></li>
<li><code><a title="litechain.contrib.OpenAIChatMessage.role" href="#litechain.contrib.OpenAIChatMessage.role">role</a></code></li>
<li><code><a title="litechain.contrib.OpenAIChatMessage.to_dict" href="#litechain.contrib.OpenAIChatMessage.to_dict">to_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="litechain.contrib.OpenAICompletionChain" href="#litechain.contrib.OpenAICompletionChain">OpenAICompletionChain</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>