"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4416],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=p(n),h=r,d=c["".concat(l,".").concat(h)]||c[h]||m[h]||o;return n?a.createElement(d,s(s({ref:t},u),{},{components:n})):a.createElement(d,s({ref:t},u))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,s=new Array(o);s[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[c]="string"==typeof e?e:r,s[1]=i;for(var p=2;p<o;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},7149:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const o={sidebar_position:5},s="Serve with FastAPI for deploying",i={unversionedId:"examples/serve-with-fastapi",id:"examples/serve-with-fastapi",title:"Serve with FastAPI for deploying",description:"Building an LLM bot and playing with it locally is simple enough, however, at some point you will want to put this bot in production, generally serving with through an API that the frontend can talk to. In this example, we are going to reuse the Simple Bot with Weather Tool example, but serve it through a FastAPI endpoint.",source:"@site/docs/examples/serve-with-fastapi.md",sourceDirName:"examples",slug:"/examples/serve-with-fastapi",permalink:"/langstream/docs/examples/serve-with-fastapi",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/examples/serve-with-fastapi.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"QA over documents",permalink:"/langstream/docs/examples/qa-over-documents"}},l={},p=[],u=(c="CodeOutputBlock",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)});var c;const m={toc:p},h="wrapper";function d(e){let{components:t,...n}=e;return(0,r.kt)(h,(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"serve-with-fastapi-for-deploying"},"Serve with FastAPI for deploying"),(0,r.kt)("p",null,"Building an LLM bot and playing with it locally is simple enough, however, at some point you will want to put this bot in production, generally serving with through an API that the frontend can talk to. In this example, we are going to reuse the ",(0,r.kt)("a",{parentName:"p",href:"./weather-bot"},"Simple Bot with Weather Tool")," example, but serve it through a FastAPI endpoint."),(0,r.kt)("p",null,"If you are not using FastAPI but something like Flask or Quartz, don't worry, the example should end up working pretty similar, we are just demonstrating it in FastAPI here because it's the more popular async-native alternative."),(0,r.kt)("p",null,"First we are going to reimplement the same bot code, with a slighly difference on the memory class, as by serving multiple users with FastAPI, we are going to need to have one memory history per user. It follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import json\nfrom typing import AsyncGenerator, List, Literal\nfrom langstream import Stream, debug, as_async_generator\n\nfrom langstream.contrib.llms.open_ai import (\n    OpenAIChatStream,\n    OpenAIChatDelta,\n    OpenAIChatMessage,\n)\n\n\nclass Memory:\n    history: List[OpenAIChatMessage]\n\n    def __init__(self) -> None:\n        self.history = []\n\n    def save_message(self, message: OpenAIChatMessage) -> OpenAIChatMessage:\n        self.history.append(message)\n        return message\n\n    def update_delta(self, delta: OpenAIChatDelta) -> OpenAIChatDelta:\n        if self.history[-1].role != delta.role and delta.role is not None:\n            self.history.append(\n                OpenAIChatMessage(\n                    role=delta.role, content=delta.content, name=delta.name\n                )\n            )\n        else:\n            self.history[-1].content += delta.content\n        return delta\n\n\ndef get_current_weather(\n    location: str, format: Literal["celsius", "fahrenheit"] = "celsius"\n) -> OpenAIChatDelta:\n    result = {\n        "location": location,\n        "forecast": "sunny",\n        "temperature": "25 C" if format == "celsius" else "77 F",\n    }\n\n    return OpenAIChatDelta(\n        role="function", name="get_current_weather", content=json.dumps(result)\n    )\n\n\n# Stream Definitions\n\n\ndef weather_bot(memory):\n    weather_stream = (\n        OpenAIChatStream[str, OpenAIChatDelta](\n            "WeatherStream",\n            lambda user_input: [\n                *memory.history,\n                memory.save_message(\n                    OpenAIChatMessage(role="user", content=user_input),\n                ),\n            ],\n            model="gpt-3.5-turbo-0613",\n            functions=[\n                {\n                    "name": "get_current_weather",\n                    "description": "Gets the current weather in a given location, use this function for any questions related to the weather",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "location": {\n                                "description": "The city to get the weather, e.g. San Francisco. Get the location from user messages",\n                                "type": "string",\n                            },\n                            "format": {\n                                "description": "A string with the full content of what the given role said",\n                                "type": "string",\n                                "enum": ("celsius", "fahrenheit"),\n                            },\n                        },\n                        "required": ["location"],\n                    },\n                }\n            ],\n            temperature=0,\n        )\n        .map(\n            # Call the function if the model produced a function call by parsing the json arguments\n            lambda delta: get_current_weather(**json.loads(delta.content))\n            if delta.role == "function" and delta.name == "get_current_weather"\n            else delta\n        )\n        .map(memory.update_delta)\n    )\n\n    function_reply_stream = OpenAIChatStream[None, OpenAIChatDelta](\n        "FunctionReplyStream",\n        lambda _: memory.history,\n        model="gpt-3.5-turbo-0613",\n        temperature=0,\n    )\n\n    async def reply_function_call(stream: AsyncGenerator[OpenAIChatDelta, None]):\n        async for output in stream:\n            if output.role == "function":\n                async for output in function_reply_stream(None):\n                    yield output\n            else:\n                yield output\n\n    weather_bot: Stream[str, str] = (\n        weather_stream.pipe(reply_function_call)\n        .map(memory.update_delta)\n        .map(lambda delta: delta.content)\n    )\n\n    return weather_bot\n')),(0,r.kt)("p",null,'Now we are going to create a FastAPI endpoint, which takes a user message, stores its history on the "database", and call the bot, returning the streamed answer from it:'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Dict\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nfrom langstream import filter_final_output\n\napp = FastAPI()\n\n\nin_memory_database: Dict[str, Memory] = {}\n\n\n@app.post("/chat")\nasync def chat(request: Request):\n    params = await request.json()\n    user_input = params.get("input")\n    user_id = params.get("user_id")\n\n    if user_id not in in_memory_database:\n        in_memory_database[user_id] = Memory()\n\n    bot = weather_bot(in_memory_database[user_id])\n    output_stream = filter_final_output(bot(user_input))\n\n    return StreamingResponse(output_stream, media_type="text/plain")\n')),(0,r.kt)("p",null,"And then start the server to make some requests to it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from uvicorn import Config, Server\nimport threading\n\nconfig = Config(app=app, host="0.0.0.0", port=8000)\nserver = Server(config)\n\nthreading.Thread(target=server.run).start()\n')),(0,r.kt)(u,{lang:"python",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    INFO:     Started server process [96682]\n    INFO:     Waiting for application startup.\n    INFO:     Application startup complete.\n    INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST http://localhost:8000/chat \\\n    -H "Content-Type: application/json" \\\n    -d \'{"user_id":"1", "input":"hi there"}\'\n')),(0,r.kt)(u,{lang:"bash",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'    INFO:     127.0.0.1:59288 - "POST /chat HTTP/1.1" 200 OK\n    Hello! How can I assist you today?\n'))),(0,r.kt)("p",null,"It's alive! It replies"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST http://localhost:8000/chat \\\n    -H "Content-Type: application/json" \\\n    -d \'{"user_id":"1", "input":"I am traveling to Amsterdam, is it hot today there?"}\'\n')),(0,r.kt)(u,{lang:"bash",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'    INFO:     127.0.0.1:59291 - "POST /chat HTTP/1.1" 200 OK\n    According to the current weather information, it is sunny in Amsterdam with a temperature of 25\xb0C. Enjoy your trip to Amsterdam!\n'))),(0,r.kt)("p",null,"Cool, and we can ask the bot about the weather too! Now, does it remember the last thing I said?"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST http://localhost:8000/chat \\\n    -H "Content-Type: application/json" \\\n    -d \'{"user_id":"1", "input":"where am I traveling to?"}\'\n')),(0,r.kt)(u,{lang:"bash",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'    INFO:     127.0.0.1:59296 - "POST /chat HTTP/1.1" 200 OK\n    You mentioned that you are traveling to Amsterdam.\n'))),(0,r.kt)("p",null,"Yes it does! What if it were a different ",(0,r.kt)("inlineCode",{parentName:"p"},"user_id")," talking to it, would it have access to the chat history as well?"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST http://localhost:8000/chat \\\n    -H "Content-Type: application/json" \\\n    -d \'{"user_id":"2", "input":"where am I traveling to?"}\'\n')),(0,r.kt)(u,{lang:"bash",mdxType:"CodeOutputBlock"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    INFO:     127.0.0.1:59298 - \"POST /chat HTTP/1.1\" 200 OK\n    I'm sorry, but as an AI assistant, I don't have access to personal information about users unless it is shared with me in the course of our conversation. Therefore, I don't know where you are traveling to.\n"))),(0,r.kt)("p",null,"No it doesn't, users have separate chat histories."),(0,r.kt)("p",null,'That\'s it, we now have a fully working API that calls our bot and answer questions about the weather, while also keeping the conversation history on the "database" separate per user, allowing to serve many users at the same time.'),(0,r.kt)("p",null,"I hope this helps you getting your LLM bot into production! If there is something you don't understand in this example, ",(0,r.kt)("a",{parentName:"p",href:"https://discord.gg/48ZM5KkKgw"},"join our discord channel")," to get help from the community!"))}d.isMDXComponent=!0}}]);