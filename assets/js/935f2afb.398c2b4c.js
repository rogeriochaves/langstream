"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Getting Started","href":"/langstream/docs/intro","docId":"intro"},{"type":"category","label":"Stream Basics","collapsible":true,"collapsed":false,"items":[{"type":"link","label":"Why Streams?","href":"/langstream/docs/stream-basics/why_streams","docId":"stream-basics/why_streams"},{"type":"link","label":"Working with Streams","href":"/langstream/docs/stream-basics/working_with_streams","docId":"stream-basics/working_with_streams"},{"type":"link","label":"Composing Streams","href":"/langstream/docs/stream-basics/composing_streams","docId":"stream-basics/composing_streams"},{"type":"link","label":"Type Signatures","href":"/langstream/docs/stream-basics/type_signatures","docId":"stream-basics/type_signatures"},{"type":"link","label":"Error Handling","href":"/langstream/docs/stream-basics/error_handling","docId":"stream-basics/error_handling"},{"type":"link","label":"Custom Streams","href":"/langstream/docs/stream-basics/custom_streams","docId":"stream-basics/custom_streams"}],"href":"/langstream/docs/stream-basics/"},{"type":"category","label":"LLMs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenAI LLMs","href":"/langstream/docs/llms/open_ai","docId":"llms/open_ai"},{"type":"link","label":"OpenAI Function Calling","href":"/langstream/docs/llms/open_ai_functions","docId":"llms/open_ai_functions"},{"type":"link","label":"GPT4All LLMs","href":"/langstream/docs/llms/gpt4all","docId":"llms/gpt4all"},{"type":"link","label":"Adding Memory","href":"/langstream/docs/llms/memory","docId":"llms/memory"},{"type":"link","label":"Zero Temperature","href":"/langstream/docs/llms/zero_temperature","docId":"llms/zero_temperature"}],"href":"/langstream/docs/llms/"},{"type":"category","label":"UI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chainlit Integration","href":"/langstream/docs/ui/chainlit","docId":"ui/chainlit"}],"href":"/langstream/docs/category/ui"},{"type":"category","label":"Code Examples","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Simple Bot with Weather Tool","href":"/langstream/docs/examples/weather-bot","docId":"examples/weather-bot"},{"type":"link","label":"Simple Bot with Weather Tool and Error Handling","href":"/langstream/docs/examples/weather-bot-error-handling","docId":"examples/weather-bot-error-handling"},{"type":"link","label":"Extracting Schema for OpenAI Functions","href":"/langstream/docs/examples/openai-function-call-extract-schema","docId":"examples/openai-function-call-extract-schema"},{"type":"link","label":"QA over documents","href":"/langstream/docs/examples/qa-over-documents","docId":"examples/qa-over-documents"},{"type":"link","label":"Serve with FastAPI for deploying","href":"/langstream/docs/examples/serve-with-fastapi","docId":"examples/serve-with-fastapi"}],"href":"/langstream/docs/examples/"},{"type":"html","value":"<a class=\'menu__link\' style=\'margin-top: 2px\' href=\'/langstream/reference/langstream/index.html\'>API Reference \ud83d\udcd6</a>"}]},"docs":{"examples/index":{"id":"examples/index","title":"Code Examples","description":"In this section you will find a list of code examples on what you can build with LangStream. Due to the focus of LangStream to keep the core light, we prefer to provide extensive examples of what you can do rather than extensive pre-made classes that do it for you, this makes it easier for you to understand how everything is working and connected together, and simply adapt to your use-case.","sidebar":"tutorialSidebar"},"examples/openai-function-call-extract-schema":{"id":"examples/openai-function-call-extract-schema","title":"Extracting Schema for OpenAI Functions","description":"In the code example below, we use the openaifunctioncall library to extract a schema to be used on OpenAIChatStream from a good old python function, so you don\'t need to write the schema yourself.","sidebar":"tutorialSidebar"},"examples/qa-over-documents":{"id":"examples/qa-over-documents","title":"QA over documents","description":"A common use case for LLMs is to reply answers to users based on search on a knowledge base, generally using a vector db under the hood.","sidebar":"tutorialSidebar"},"examples/serve-with-fastapi":{"id":"examples/serve-with-fastapi","title":"Serve with FastAPI for deploying","description":"Building an LLM bot and playing with it locally is simple enough, however, at some point you will want to put this bot in production, generally serving with through an API that the frontend can talk to. In this example, we are going to reuse the Simple Bot with Weather Tool example, but serve it through a FastAPI endpoint.","sidebar":"tutorialSidebar"},"examples/weather-bot":{"id":"examples/weather-bot","title":"Simple Bot with Weather Tool","description":"Below is a code example of a bot you can talk too which has the ability of checking the weather, it has memory, it is using OpenAI functions, and it streams its outputs:","sidebar":"tutorialSidebar"},"examples/weather-bot-error-handling":{"id":"examples/weather-bot-error-handling","title":"Simple Bot with Weather Tool and Error Handling","description":"The example below is similar to the Simple Bot with Weather Tool example, but here we add on_error for error handling in case something went wrong when calling the weather function. What we do is simply inject the error back into the LLM, so it can figure out itself what is missing, which in example below is the location field, and ask the user for more input","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Getting Started","description":"LangStream is a lighter alternative to LangChain for building LLMs application, instead of having a massive amount of features and classes, LangStream focuses on having a single small core, that is easy to learn, easy to adapt, well documented, fully typed and truly composable, with Streams instead of chains as the building block.","sidebar":"tutorialSidebar"},"llms/gpt4all":{"id":"llms/gpt4all","title":"GPT4All LLMs","description":"LLMs require a lot of GPU to run properly make it hard for the common folk to set one up locally. Fortunately, the folks at GPT4All are doing an excellent job in really reducing those models with various techniques, and speeding them up to run on CPUs everywhere with no issues. LangStream also provides a thin wrapper for them, and since it\'s local, no API keys are required.","sidebar":"tutorialSidebar"},"llms/index":{"id":"llms/index","title":"LLMs","description":"Large Language Models like GPT-4 is the whole reason LangStream exists, we want to build on top of LLMs to construct an application. After learning the Stream Basics, it should be clear how you can wrap any LLM in a Stream, you just need to produce an AsyncGenerator out of their output. However, LangStream already come with some LLM streams out of the box to make it easier.","sidebar":"tutorialSidebar"},"llms/memory":{"id":"llms/memory","title":"Adding Memory","description":"LLMs are stateless, and LangStream also strive to be as stateless as possible, which makes things easier to reason about. However, this means your Streams will have no memory by default.","sidebar":"tutorialSidebar"},"llms/open_ai":{"id":"llms/open_ai","title":"OpenAI LLMs","description":"OpenAI took the world by storm with the launch of ChatGPT and GPT-4, at the point of this writing, they are still the smartest LLMs out there. To use them, first you will need to get an API key from OpenAI, and export it with:","sidebar":"tutorialSidebar"},"llms/open_ai_functions":{"id":"llms/open_ai_functions","title":"OpenAI Function Calling","description":"By default, LLMs take text as input, and product text as output, but when we are building LLM applications, many times we want some specific outputs from the LLM, or to fork the execution flow to take the user in another direction. One way to do that, is to ask the LLM to produce a JSON, and the try to parse that JSON. Problem is, often times this JSON can be invalid, and it\'s a bit hassle to work with it.","sidebar":"tutorialSidebar"},"llms/zero_temperature":{"id":"llms/zero_temperature","title":"Zero Temperature","description":"LLMs work by predicting the next token, the token that gets chosen is the one with the highest probability to be the next one, calculated given the input and all the model training, this is the magic of LLMs in short.","sidebar":"tutorialSidebar"},"stream-basics/composing_streams":{"id":"stream-basics/composing_streams","title":"Composing Streams","description":"If you are familiar with Functional Programming, the Stream follows the Monad Laws, this ensures they are composable to build complex application following the Category Theory definitions. Our goal on building LangStream was always to make it truly composable, and this is the best abstraction we know for the job, so we adopted it.","sidebar":"tutorialSidebar"},"stream-basics/custom_streams":{"id":"stream-basics/custom_streams","title":"Custom Streams","description":"If you have been following the guides, now you know how to create streams, how to compose them together, and everything, however, what if you want to change the core behaviour of the stream, how do you do it? Well, turns out, there are no \\"custom streams\\" really, it\'s all just composition.","sidebar":"tutorialSidebar"},"stream-basics/error_handling":{"id":"stream-basics/error_handling","title":"Error Handling","description":"When dealing with LLMs, errors are actually quite common, be it a connection failure on calling APIs or invalid parameters hallucinated by the model, so it\'s important to think carefully on how to handle exceptions.","sidebar":"tutorialSidebar"},"stream-basics/index":{"id":"stream-basics/index","title":"Stream Basics","description":"The Stream is the main building block of LangStream, you compose streams together to build your LLM application.","sidebar":"tutorialSidebar"},"stream-basics/type_signatures":{"id":"stream-basics/type_signatures","title":"Type Signatures","description":"In the recent years, Python has been expanding the support for type hints, which helps a lot during development to catch bugs from types that should not be there, and even detecting Nones before they happen.","sidebar":"tutorialSidebar"},"stream-basics/why_streams":{"id":"stream-basics/why_streams","title":"Why Streams?","description":"For a visualization on streaming vs blocking, take a look at vercel docs on AI streaming, they have a nice animation there","sidebar":"tutorialSidebar"},"stream-basics/working_with_streams":{"id":"stream-basics/working_with_streams","title":"Working with Streams","description":"By default, all LLMs generate a stream of tokens:","sidebar":"tutorialSidebar"},"ui/chainlit":{"id":"ui/chainlit","title":"Chainlit Integration","description":"Chainlit is a UI that gives you a ChatGPT like interface for your streams, it is very easy to set up, it has a slick UI, and it allows you to visualize the intermediary steps, so it\'s great for development!","sidebar":"tutorialSidebar"}}}')}}]);