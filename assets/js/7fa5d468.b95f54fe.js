"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4943],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>h});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},m=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=p(n),d=a,h=c["".concat(s,".").concat(d)]||c[d]||u[d]||o;return n?r.createElement(h,l(l({ref:t},m),{},{components:n})):r.createElement(h,l({ref:t},m))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,l=new Array(o);l[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[c]="string"==typeof e?e:a,l[1]=i;for(var p=2;p<o;p++)l[p]=n[p];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7702:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const o={sidebar_position:4},l="GPT4All LLMs",i={unversionedId:"llms/gpt4all",id:"llms/gpt4all",title:"GPT4All LLMs",description:"LLMs require a lot of GPU to run properly make it hard for the common folk to set one up locally. Fortunately, the folks at GPT4All are doing an excellent job in really reducing those models with various techniques, and speeding them up to run on CPUs everywhere with no issues. LangStream also provides a thin wrapper for them, and since it's local, no API keys are required.",source:"@site/docs/llms/gpt4all.md",sourceDirName:"llms",slug:"/llms/gpt4all",permalink:"/langstream/docs/llms/gpt4all",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llms/gpt4all.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"OpenAI Function Calling",permalink:"/langstream/docs/llms/open_ai_functions"},next:{title:"Lite LLM (Azure, Anthropic, etc)",permalink:"/langstream/docs/llms/lite_llm"}},s={},p=[],m={toc:p},c="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"gpt4all-llms"},"GPT4All LLMs"),(0,a.kt)("p",null,"LLMs require a lot of GPU to run properly make it hard for the common folk to set one up locally. Fortunately, the folks at ",(0,a.kt)("a",{parentName:"p",href:"https://gpt4all.io/index.html"},"GPT4All")," are doing an excellent job in really reducing those models with various techniques, and speeding them up to run on CPUs everywhere with no issues. LangStream also provides a thin wrapper for them, and since it's local, no API keys are required."),(0,a.kt)("h1",{id:"gpt4allstream"},"GPT4AllStream"),(0,a.kt)("p",null,"You can use a ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/langstream/contrib/index.html#langstream.contrib.GPT4AllStream"},(0,a.kt)("inlineCode",{parentName:"a"},"GPT4AllStream"))," like this:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from langstream import join_final_output\nfrom langstream.contrib import GPT4AllStream\n\ngreet_stream = GPT4AllStream[str, str](\n    "GreetingStream",\n    lambda name: f"### User: Hello little person that lives in my CPU, my name is {name}. How is it going?\\\\n\\\\n### Response:",\n    model="orca-mini-3b.ggmlv3.q4_0.bin",\n    temperature=0,\n)\n\nawait join_final_output(greet_stream("Alice"))\n#=> " I\'m doing well, thank you for asking! How about you?"\n')),(0,a.kt)("p",null,"The first time you run it, it will download the model you are using (in this case ",(0,a.kt)("inlineCode",{parentName:"p"},"orca-mini-3b.ggmlv3.q4_0.bin"),"), you can also specify a pathname there if you wish, you can check out all GPT4All available models ",(0,a.kt)("a",{parentName:"p",href:"https://gpt4all.io/index.html"},"on their website")," on Model Explorer."),(0,a.kt)("p",null,"Then, you might have noticed the prompt is just a string, but we do have roles markers inside it, with ",(0,a.kt)("inlineCode",{parentName:"p"},"### User:")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"### Response:"),", with two ",(0,a.kt)("inlineCode",{parentName:"p"},"\\n\\n")," line breaks in between. This is how GPT4All models were trained, and you can use this same patterns to keep the roles behaviour."),(0,a.kt)("p",null,"Also, in the example we used ",(0,a.kt)("inlineCode",{parentName:"p"},"temperature=0"),", for stability as explained ",(0,a.kt)("a",{parentName:"p",href:"/docs/llms/zero_temperature"},"here"),", but ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/langstream/contrib/index.html#langstream.contrib.GPT4AllStream"},(0,a.kt)("inlineCode",{parentName:"a"},"GPT4AllStream"))," has many more parameters you can adjust that can work better depending on the model you are choosing, check them out on ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/langstream/contrib/index.html#langstream.contrib.GPT4AllStream"},"the reference"),"."))}u.isMDXComponent=!0}}]);