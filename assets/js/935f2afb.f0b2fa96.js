"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Getting Started","href":"/litechain/docs/intro","docId":"intro"},{"type":"category","label":"Chain Basics","collapsible":true,"collapsed":false,"items":[{"type":"link","label":"Why Streams?","href":"/litechain/docs/chain-basics/why_streams","docId":"chain-basics/why_streams"},{"type":"link","label":"Working with Streams","href":"/litechain/docs/chain-basics/working_with_streams","docId":"chain-basics/working_with_streams"},{"type":"link","label":"Composing Chains","href":"/litechain/docs/chain-basics/composing_chains","docId":"chain-basics/composing_chains"},{"type":"link","label":"Type Signatures","href":"/litechain/docs/chain-basics/type_signatures","docId":"chain-basics/type_signatures"}],"href":"/litechain/docs/chain-basics/"},{"type":"category","label":"LLMs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenAI LLMs","href":"/litechain/docs/llms/open_ai","docId":"llms/open_ai"},{"type":"link","label":"OpenAI Function Calling","href":"/litechain/docs/llms/open_ai_functions","docId":"llms/open_ai_functions"},{"type":"link","label":"GPT4All LLMs","href":"/litechain/docs/llms/gpt4all","docId":"llms/gpt4all"},{"type":"link","label":"Adding Memory","href":"/litechain/docs/llms/memory","docId":"llms/memory"},{"type":"link","label":"Zero Temperature","href":"/litechain/docs/llms/zero_temperature","docId":"llms/zero_temperature"}],"href":"/litechain/docs/llms/"},{"type":"category","label":"UI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chainlit Integration","href":"/litechain/docs/ui/chainlit","docId":"ui/chainlit"}],"href":"/litechain/docs/category/ui"},{"type":"category","label":"Code Examples","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Simple Bot with Weather Tool","href":"/litechain/docs/examples/weather-bot","docId":"examples/weather-bot"},{"type":"link","label":"Extracting Schema for OpenAI Functions","href":"/litechain/docs/examples/openai-function-call-extract-schema","docId":"examples/openai-function-call-extract-schema"}],"href":"/litechain/docs/examples/"},{"type":"html","value":"<a class=\'menu__link\' style=\'margin-top: 2px\' href=\'/litechain/reference/litechain/index.html\'>API Reference \ud83d\udcd6</a>"}]},"docs":{"chain-basics/composing_chains":{"id":"chain-basics/composing_chains","title":"Composing Chains","description":"If you are familiar with Functional Programming, the Chain follows the Monad Laws, this ensures they are composable to build complex application following the Category Theory definitions. Our goal on building LiteChain was always to make it truly composable, and this is the best abstraction we know for the job, so we adopted it.","sidebar":"tutorialSidebar"},"chain-basics/index":{"id":"chain-basics/index","title":"Chain Basics","description":"The Chain is the main building block of LiteChain, you compose chains together to build your LLM application.","sidebar":"tutorialSidebar"},"chain-basics/type_signatures":{"id":"chain-basics/type_signatures","title":"Type Signatures","description":"In the recent years, Python has been expanding the support for type hints, which helps a lot during development to catch bugs from types that should not be there, and even detecting Nones before they happen.","sidebar":"tutorialSidebar"},"chain-basics/why_streams":{"id":"chain-basics/why_streams","title":"Why Streams?","description":"For a visualization on streaming vs blocking, take a look at vercel docs on AI streaming, they have a nice animation there","sidebar":"tutorialSidebar"},"chain-basics/working_with_streams":{"id":"chain-basics/working_with_streams","title":"Working with Streams","description":"By default, all LLMs generate a stream of tokens:","sidebar":"tutorialSidebar"},"examples/index":{"id":"examples/index","title":"Code Examples","description":"In this section you will find a list of code examples on what you can build with LiteChain. Due to the focus of LiteChain to keep the core light, we prefer to provide extensive examples of what you can do rather than extensive pre-made classes that do it for you, this makes it easier for you to understand how everything is working and connected together, and simply adapt to your use-case.","sidebar":"tutorialSidebar"},"examples/openai-function-call-extract-schema":{"id":"examples/openai-function-call-extract-schema","title":"Extracting Schema for OpenAI Functions","description":"In the code example below, we use the openaifunctioncall library to extract a schema to be used on OpenAIChatChain from a good old python function, so you don\'t need to write the schema yourself.","sidebar":"tutorialSidebar"},"examples/weather-bot":{"id":"examples/weather-bot","title":"Simple Bot with Weather Tool","description":"Below is a code example of a bot you can talk too which has the ability of checking the weather, it has memory, it is using OpenAI functions, and it streams its outputs:","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Getting Started","description":"LiteChain is a lighter alternative to LangChain for building LLMs application, instead of having a massive amount of features and classes, LiteChain focuses on having a single small core, that is easy to learn, easy to adapt, well documented, fully typed and truly composable.","sidebar":"tutorialSidebar"},"llms/gpt4all":{"id":"llms/gpt4all","title":"GPT4All LLMs","description":"LLMs require a lot of GPU to run properly make it hard for the common folk to set one up locally. Fortunately, the folks at GPT4All are doing an excellent job in really reducing those models with various techniques, and speeding them up to run on CPUs everywhere with no issues. LiteChain also provides a thin wrapper for them, and since it\'s local, no API keys are required.","sidebar":"tutorialSidebar"},"llms/index":{"id":"llms/index","title":"LLMs","description":"Large Language Models like GPT-4 is the whole reason LiteChain exists, we want to build on top of LLMs to construct an application. After learning the Chain Basics, it should be clear how you can wrap any LLM in a Chain, you just need to produce an AsyncGenerator out of their output. However, LiteChain already come with some LLM chains out of the box to make it easier.","sidebar":"tutorialSidebar"},"llms/memory":{"id":"llms/memory","title":"Adding Memory","description":"LLMs are stateless, and LiteChain also strive to be as stateless as possible, which makes things easier to reason about. However, this means your Chains will have no memory by default.","sidebar":"tutorialSidebar"},"llms/open_ai":{"id":"llms/open_ai","title":"OpenAI LLMs","description":"OpenAI took the world by storm with the launch of ChatGPT and GPT-4, at the point of this writing, they are still the smartest LLMs out there. To use them, first you will need to get an API key from OpenAI, and export it with:","sidebar":"tutorialSidebar"},"llms/open_ai_functions":{"id":"llms/open_ai_functions","title":"OpenAI Function Calling","description":"By default, LLMs take text as input, and product text as output, but when we are building LLM applications, many times we want some specific outputs from the LLM, or to fork the execution flow to take the user in another direction. One way to do that, is to ask the LLM to produce a JSON, and the try to parse that JSON. Problem is, often times this JSON can be invalid, and it\'s a bit hassle to work with it.","sidebar":"tutorialSidebar"},"llms/zero_temperature":{"id":"llms/zero_temperature","title":"Zero Temperature","description":"LLMs work by predicting the next token, the token that gets chosen is the one with the highest probability to be the next one, calculated given the input and all the model training, this is the magic of LLMs in short.","sidebar":"tutorialSidebar"},"ui/chainlit":{"id":"ui/chainlit","title":"Chainlit Integration","description":"Chainlit is a UI that gives you a ChatGPT like interface for your chains, it is very easy to set up, it has a slick UI, and it allows you to visualize the intermediary steps, so it\'s great for development!","sidebar":"tutorialSidebar"}}}')}}]);